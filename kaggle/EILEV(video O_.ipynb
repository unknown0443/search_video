{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11037554,"sourceType":"datasetVersion","datasetId":6874872},{"sourceId":11037559,"sourceType":"datasetVersion","datasetId":6874876},{"sourceId":11039466,"sourceType":"datasetVersion","datasetId":6876224},{"sourceId":11067295,"sourceType":"datasetVersion","datasetId":6896475}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install decord -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2ForConditionalGeneration, AutoProcessor\nimport torch\nfrom PIL import Image\nimport os\nimport sys\nimport decord\nfrom decord import VideoReader, cpu\nfrom transformers import BitsAndBytesConfig\nimport numpy as np\n# sys.path.append(\"/kaggle/input/eilev-git/EILEV-main/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kimmin1253/testvideo-ive\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /kaggle/input/eilev-git/EILEV-main/eilev/\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from accelerate import infer_auto_device_map\n\n# âœ… ëª¨ë¸ ë¡œë“œ ì‹œ `device_map` ëª…ì‹œì ìœ¼ë¡œ ì§€ì •\ndevice_map = infer_auto_device_map(model, max_memory={0: \"8GB\", 1: \"8GB\"}, no_split_module_classes=[\"language_model\"])\n\n# âœ… EILeV ëª¨ë¸ ê²½ë¡œ\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# âœ… ëª¨ë¸ ë¡œë“œ (device_map ìˆ˜ì •)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map= device_map, #{\"\": 0},  # ğŸ”¥ ëª…í™•í•˜ê²Œ GPU 0ë²ˆì— ë¡œë“œ\n    quantization_config = BitsAndBytesConfig(load_in_8bit=True),\n    torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ìµœì í™”\n)\n# âœ… `language_model`ì´ device_mapì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ë¶„ë°°\n# model = dispatch_model(model, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"âœ… Loaded Model Class: {type(model)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… BLIP-2ì˜ ì…ë ¥ ì²˜ë¦¬ ë°©ì‹ ì ìš©\n# âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ì…ë ¥ ë³€í™˜\n\nfrom eilev.model.utils import process\n\n# âœ… ë¹„ë””ì˜¤(mp4) íŒŒì¼ ê²½ë¡œ\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\n\n# âœ… OpenCVë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¡œë“œ\ncap = cv2.VideoCapture(video_path)\nframes = []\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nnum_frames = 16  # ì´ˆë‹¹ 1í”„ë ˆì„ì”© ì´ 16ê°œ ìƒ˜í”Œë§\n\n# âœ… í”„ë ˆì„ ì¸ë±ìŠ¤ ì„ íƒ\nindices = np.linspace(0, frame_count - 1, num_frames).astype(int)\n\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCVëŠ” BGR â†’ RGB ë³€í™˜ í•„ìš”\n    frames.append(frame)\n\ncap.release()  # ğŸ”¥ ë¹„ë””ì˜¤ íŒŒì¼ ë‹«ê¸°\n\n\n# í”„ë ˆì„ì„ Tensorë¡œ ë³€í™˜: ìµœì¢… shape: (1, 3, num_frames, 224, 224)\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\n# --------------------------\n# 3. ë¹„ë””ì˜¤ ì…ë ¥ ì „ì²˜ë¦¬ (process í•¨ìˆ˜ ì‚¬ìš©)\n# --------------------------\n# ì—¬ê¸°ì„œ 'text'ëŠ” ë‚˜ì¤‘ì— í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ì…ë ¥ë  í…ìŠ¤íŠ¸ ë¶€ë¶„ì„ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n# í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ì§ì ‘ input_idsë¥¼ êµ¬ì„±í•  ì˜ˆì •ì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\ninputs_video = process(processor, video=frames_tensor)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# 4. ì…ë ¥ ì‹œí€€ìŠ¤ì™€ video_input_mask ìƒì„± (EILeV ë°©ì‹)\n# --------------------------\n# tokenizer ê°€ì ¸ì˜¤ê¸°\ntokenizer = processor.tokenizer\n\n# í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ë¹„ë””ì˜¤ í† í° ìë¦¬ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë‘ í¬í•¨\n# ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸: \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n\n# í•„ìš”í•œ í† í°: bos_token, video placeholders (num_query_tokens), newline, prompt tokens\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# êµ¬ì„±:\n#   [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\ninput_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids], dtype=torch.long).to(device)  # shape: (1, L)\nnum_tokens = input_ids.shape[1]\n\n# video_input_mask: True for video token positions, False for others.\n# bos í† í°: 0, video placeholders: 1, newline + prompt: 0\nvideo_input_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask], dtype=torch.bool).to(device)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# 5. Combine ì…ë ¥ (ë¹„ë””ì˜¤ í”½ì…€ ê°’)\n# --------------------------\n# inputs_videoëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ, íŠ¹íˆ \"pixel_values\"ê°€ í¬í•¨ë¨.\ninputs = {}\ninputs[\"input_ids\"] = input_ids\ninputs[\"pixel_values\"] = inputs_video[\"pixel_values\"].to(device)\n\n# ë””ë²„ê¹… í”„ë¦°íŠ¸\nprint(f\"ğŸ” input_ids.shape: {inputs['input_ids'].shape}\")      # ì˜ˆ: (1, L) where L = 1+32+1+len(prompt_tokens)\nprint(f\"ğŸ” video_input_mask.shape: {video_input_mask.shape}\")    # (1, L)\nprint(f\"ğŸ” video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\nprint(f\"ğŸ” pixel_values.shape: {inputs['pixel_values'].shape}\")    # (1, 3, num_frames, 224, 224)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… GPU ì´ë™\ndevice = model.device\ninputs = {key: val.to(device) for key, val in inputs.items()}\nvideo_input_mask = video_input_mask.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… EILeV ëª¨ë¸ ì‹¤í–‰\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,  # ğŸ“Œ í•„ìˆ˜ ì…ë ¥ ì¶”ê°€\n        max_new_tokens=30,\n        num_beams=4,\n        repetition_penalty=1.5,\n    )\n\n# âœ… ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\n# âœ… ê²°ê³¼ ì¶œë ¥\nprint(\"\\nğŸ¬ Video Description Generated:\")\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… ê¸°ì¡´ ë°©ì‹: ì „ì²´ í”„ë ˆì„ì„ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ ì„¤ëª… ìƒì„±\nwith torch.no_grad():  # ğŸ”¥ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ìµœì í™”)\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=pixel_values,  # ğŸ”¥ 5D í…ì„œë¡œ ë³€í™˜ëœ ë¹„ë””ì˜¤ ì…ë ¥\n        video_input_mask=video_input_mask,  # âœ… í•„ìˆ˜ ì¶”ê°€\n        max_new_tokens=80,  # ğŸ”¥ ì„¤ëª… ê¸¸ì´ ì¡°ì •\n        num_beams=4,  # ğŸ”¥ ë¹” ì„œì¹˜ í™œìš©\n        repetition_penalty=1.3,  # ğŸ”¥ ë°˜ë³µ ì¤„ì´ê¸°\n    )\n\n# âœ… ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\nprint(\"\\nğŸ¬ Video Description Generated:\")\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frame_paths = [\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0000.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0029.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0058.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0087.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0116.jpg\",\n]\nframes = [Image.open(path).convert(\"RGB\") for path in frame_paths]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… BLIP-2ì˜ ìƒˆë¡œìš´ ì…ë ¥ ì²˜ë¦¬ ë°©ì‹ ì ìš©\nprompt = \"Question: Describe in detail what actions are taking place in the video.\\nAnswer:\"\n\ninputs = processor(\n    images=frames,\n    text=[prompt] * len(frames),  # ğŸ”¥ ëª¨ë“  í”„ë ˆì„ì— ê°™ì€ í”„ë¡¬í”„íŠ¸ ì ìš©\n    return_tensors=\"pt\",\n    padding=True,  # ğŸ”¥ íŒ¨ë”© ì¶”ê°€í•˜ì—¬ í¬ê¸° ë§ì¶”ê¸°\n)\n\n# ğŸš€ Debugging: ë³€í™˜ëœ ì…ë ¥ ë°ì´í„° í¬ê¸° í™•ì¸\nprint(f\"Input IDs shape: {inputs['input_ids'].shape}\")  # (batch_size, sequence_length)\nprint(f\"Pixel Values shape: {inputs['pixel_values'].shape}\")  # (batch_size, 3, 224, 224)\n# âœ… GPU ì´ë™\ninputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n\nprint(\"âœ… ì…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… ê¸°ì¡´ ë°©ì‹: ì „ì²´ í”„ë ˆì„ì„ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ ì„¤ëª… ìƒì„±\nwith torch.no_grad():  # ğŸ”¥ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ìµœì í™”)\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=80,  # ê¸¸ì´ ì¡°ì • ìœ ì§€\n        num_beams=4,  # ì ì ˆí•œ ë¹” ì„œì¹˜ ìœ ì§€\n        repetition_penalty=1.3,  # ğŸ”¥ ë°˜ë³µ ë¬¸ì¥ ì¤„ì´ê¸°\n    )\n\n# âœ… ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##################################################################################","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\n\n# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILeV ì „ìš©)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\n# EILeV ì „ìš© í´ë˜ìŠ¤ë¡œ ë¡œë“œ (8-bit, GPU ìë™ ë°°ë¶„)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n# ëª¨ë¸ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨ (8-bit ì–‘ìí™” ì‹œ .to(device) ë¶ˆí•„ìš”)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. ë¹„ë””ì˜¤(mp4) íŒŒì¼ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ (OpenCV ì‚¬ìš©)\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\nframes = []\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nnum_frames = 16  # ì˜ˆ: ì´ˆë‹¹ 1í”„ë ˆì„ì”© ì´ 16ê°œ ìƒ˜í”Œë§\n\n\n\n# TODO\n=====================================================\nì´ˆë‹¹ 2 í”„ë ˆì„ì”© num_frames= 10 (5ì´ˆ) ì§œë¦¬ frames ìƒì„±\n\ntotal_frames_list = []\n\n10_frames_list = 5ì´ˆì”© ìë¥´ê¸°\n\nfor 24ë²ˆ ë™ì•ˆ:\ntotal_frames_list.append(10_frames_list)\n\n\nresults = []\n#EILeV ëª¨ë¸ 24ë²ˆ í˜¸ì¶œ:                         \nfor 10_frames_list in total_frames_list:\n    10_frames_listë¥¼ ë„£ê¸°\n    result = {}\n    blip_result = videoblipmodel(input_is, mask, )\n\n    result['time'] = time ì •ë³´\n    result['caption'] = blip_result\n    \n    results.append(result)\n\nresults íŒŒì¼ë¡œ timeframeê³¼ í•¨ê»˜ íŒŒì¼ë¡œ ì“°ê¸°\n=====================================================\n\n\n\nindices = np.linspace(0, frame_count - 1, num=num_frames).astype(int)\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    # OpenCVëŠ” BGR â†’ RGB ë³€í™˜ í•„ìš”\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frames.append(frame)\ncap.release()\n\n# í”„ë ˆì„ì„ Tensorë¡œ ë³€í™˜ (ìµœì¢… shape: [1, 3, num_frames, 224, 224])\n# (num_frames, H, W, C) â†’ (1, C, num_frames, H, W)\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\n# ==================================================\n# 3. ë¹„ë””ì˜¤ ì…ë ¥ ì „ì²˜ë¦¬ (process() í•¨ìˆ˜ ì‚¬ìš©)\n# ==================================================\n# ì—¬ê¸°ì„œëŠ” ë¹„ë””ì˜¤ ë¶€ë¶„ë§Œ ì²˜ë¦¬í•˜ì—¬ pixel_valuesë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\ninputs_video = process(processor, video=frames_tensor, text=None)\n# inputs_video[\"pixel_values\"]ì˜ shape: (1, 3, num_frames, 224, 224)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 4. ì…ë ¥ ì‹œí€€ìŠ¤ ë° video_input_mask êµ¬ì„± (EILeV í˜•ì‹)\n# ==================================================\n# EILeVëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì— ë¹„ë””ì˜¤ ê´€ë ¨ ìë¦¬(ì¿¼ë¦¬ í† í°)ë¥¼ ë¨¼ì € ë°°ì¹˜í•œ í›„, í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\nprompt = \"Question: Describe in detail what actions are taking place in the video.\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad] * num_query_tokens + [newline] + prompt_tokens\ninput_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids], dtype=torch.long).to(device)  # shape: (1, L)\nnum_tokens = input_ids.shape[1]\n\n# video_input_mask: bos í† í° â†’ 0, ë¹„ë””ì˜¤ ìë¦¬ (ì¿¼ë¦¬ í† í°) â†’ 1, ë‚˜ë¨¸ì§€(í…ìŠ¤íŠ¸) â†’ 0\nvideo_input_mask_list = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n\n# ==================================================\n# 5. ìµœì¢… ì…ë ¥ êµ¬ì„±\n# ==================================================\ninputs = {}\ninputs[\"input_ids\"] = input_ids\ninputs[\"pixel_values\"] = inputs_video[\"pixel_values\"].to(device)\n\n# ==================================================\n# 6. ë””ë²„ê¹… í”„ë¦°íŠ¸\n# ==================================================\nprint(f\"ğŸ” input_ids.shape: {inputs['input_ids'].shape}\")          # ì˜ˆ: (1, L) where L = 1 + 32 + 1 + len(prompt_tokens)\nprint(f\"ğŸ” pixel_values.shape: {inputs['pixel_values'].shape}\")        # (1, 3, num_frames, 224, 224)\nprint(f\"ğŸ” video_input_mask.shape: {video_input_mask.shape}\")          # (1, L)\nprint(f\"ğŸ” video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\n\n# ==================================================\n# 7. ëª¨ë¸ ì‹¤í–‰ (EILeV)\n# ==================================================\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,  # í•„ìˆ˜ ì…ë ¥: shape (1, L)\n        max_new_tokens=80,\n        num_beams=4,\n        repetition_penalty=1.3,\n    )\n\n# ==================================================\n# 8. ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© ë° ì¶œë ¥\n# ==================================================\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\nprint(\"\\nğŸ¬ Video Description Generated:\")\nprint(generated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\n\n# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILeV ì „ìš©)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\n# EILeV ì „ìš© ëª¨ë¸ ë¡œë“œ (8-bit, GPU ìë™ ë°°ë¶„)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. ë¹„ë””ì˜¤ ì „ì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\n\n# ==================================================\n# 3. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„±\n# ==================================================\nsegment_duration = 5  # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\nframes_per_segment = int(fps * segment_duration)\n\nmetadata = []  # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n\nfor start_frame in range(0, total_frames, frames_per_segment):\n    # ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œì‘ê³¼ ë ê³„ì‚°\n    end_frame = min(start_frame + frames_per_segment, total_frames)\n    num_segment_frames = end_frame - start_frame\n    \n    # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ì—ì„œ ê· ë“±í•˜ê²Œ í”„ë ˆì„ ì„ íƒ\n    cap = cv2.VideoCapture(video_path)\n    indices = np.linspace(start_frame, end_frame - 1, num=num_segment_frames).astype(int)\n    segment_frames = []\n    for i in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        segment_frames.append(frame)\n    cap.release()\n    \n    if len(segment_frames) == 0:\n        continue\n    \n    # -------------------------------\n    # 3-1. í”„ë ˆì„ ì „ì²˜ë¦¬: Tensor ë³€í™˜\n    # -------------------------------\n    # (num_frames, H, W, C) â†’ (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    # -------------------------------\n    # 3-2. ë¹„ë””ì˜¤ ì „ì²˜ë¦¬: process() í•¨ìˆ˜ ì‚¬ìš©\n    # -------------------------------\n    inputs_video = process(processor, video=segment_tensor, text=None)\n    \n    # -------------------------------\n    # 3-3. ì…ë ¥ ì‹œí€€ìŠ¤ ë° video_input_mask êµ¬ì„±\n    # -------------------------------\n    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ë‹¨ì¼ Q&A ìŒ\n    prompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\n    prompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n    \n    bos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n    pad_token = tokenizer.pad_token_id\n    newline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n    num_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n    \n    # ì…ë ¥ ì‹œí€€ìŠ¤: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\n    input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\n    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n    num_tokens = input_ids.shape[1]\n    \n    # video_input_mask: bos â†’ 0, ë¹„ë””ì˜¤ ìë¦¬ (ì¿¼ë¦¬ í† í°) â†’ 1, ë‚˜ë¨¸ì§€ (í…ìŠ¤íŠ¸) â†’ 0\n    video_input_mask_list = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\n    video_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n    \n    # -------------------------------\n    # 3-4. ìµœì¢… ì…ë ¥ êµ¬ì„±\n    # -------------------------------\n    inputs = {\n        \"input_ids\": input_ids,\n        \"pixel_values\": inputs_video[\"pixel_values\"].to(device)\n    }\n    \n    # ë””ë²„ê¹… í”„ë¦°íŠ¸\n    print(f\"ğŸ” Segment starting at {start_frame/fps:.1f}s:\")\n    print(f\"   input_ids.shape: {inputs['input_ids'].shape}\")\n    print(f\"   pixel_values.shape: {inputs['pixel_values'].shape}\")\n    print(f\"   video_input_mask.shape: {video_input_mask.shape}\")\n    print(f\"   video_input_mask sum: {video_input_mask.sum().item()}\")\n    \n    # -------------------------------\n    # 3-5. ëª¨ë¸ ì‹¤í–‰ ë° ìº¡ì…˜ ìƒì„±\n    # -------------------------------\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            video_input_mask=video_input_mask,\n            max_new_tokens=80,\n            num_beams=4,\n            repetition_penalty=1.3,\n        )\n    caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n    \n    # ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ê³¼ ìº¡ì…˜ ì €ì¥\n    metadata.append({\n        \"start_time\": start_frame / fps,\n        \"end_time\": end_frame / fps,\n        \"caption\": caption\n    })\n\n# ==================================================\n# 4. ê²°ê³¼ ì¶œë ¥: ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ìº¡ì…˜\n# ==================================================\nfor data in metadata:\n    print(f\"Segment {data['start_time']:.1f}s - {data['end_time']:.1f}s: {data['caption']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"===========================ë¹„ë””ì˜¤ 5ì´ˆë§ˆë‹¤ ì„¤ëª…ìƒì„±===============================\nkpyu/eilev-blip2-opt-2.7bì ìš©\n","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILeV ì „ìš©)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EILeV ì „ìš© ëª¨ë¸ ë¡œë“œ (8-bit, GPU ìë™ ë°°ë¶„)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n# ëª¨ë¸ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨ (8-bit ì–‘ìí™” ì‹œ .to(device) ë¶ˆí•„ìš”)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 2. ë¹„ë””ì˜¤ ì „ì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\nprint(f\"Total frames: {total_frames}, FPS: {fps}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 3. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„±\n# ==================================================\nsegment_duration = 5  # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\nframes_per_segment = int(fps * segment_duration)\n\n# í”„ë¡¬í”„íŠ¸ ë° ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„± (ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì— ë™ì¼)\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# ì…ë ¥ ì‹œí€€ìŠ¤: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\n# video_input_mask: bos í† í° â†’ 0, video ìë¦¬ (ì¿¼ë¦¬ í† í°) â†’ 1, ë‚˜ë¨¸ì§€ (í…ìŠ¤íŠ¸) â†’ 0\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸\nmetadata = []\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 4. ì„¸ê·¸ë¨¼íŠ¸ë³„ ìº¡ì…˜ ìƒì„±\n# ==================================================\ncap = cv2.VideoCapture(video_path)\nfor start_frame in range(0, total_frames, frames_per_segment):\n    end_frame = min(start_frame + frames_per_segment, total_frames)\n    # ì„¸ê·¸ë¨¼íŠ¸ í”„ë ˆì„ ì¶”ì¶œ\n    segment_frames = []\n    for i in range(start_frame, end_frame):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB\n        segment_frames.append(frame)\n    if len(segment_frames) == 0:\n        continue\n    \n    # í”„ë ˆì„ì„ Tensorë¡œ ë³€í™˜: (num_frames, H, W, C) â†’ (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    # EILeV ê³µì‹ process() í•¨ìˆ˜ë¡œ ë¹„ë””ì˜¤ ì…ë ¥ ì „ì²˜ë¦¬ (ì‹œê°„ ì •ë³´ ìœ ì§€)\n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)  # (1, 3, num_frames, 224, 224)\n    \n    # ìµœì¢… ì…ë ¥ êµ¬ì„±: ë™ì¼í•œ í…ìŠ¤íŠ¸ ì…ë ¥ (base_input_ids)ì™€ base_video_mask ì‚¬ìš©\n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask  # shape: (1, num_tokens)\n    \n    # ë””ë²„ê¹… í”„ë¦°íŠ¸ (ê° ì„¸ê·¸ë¨¼íŠ¸ë³„)\n    print(f\"ğŸ” Segment starting at {start_frame/fps:.1f}s:\")\n    print(f\"   input_ids.shape: {inputs_seg['input_ids'].shape}\")    # (1, num_tokens)\n    print(f\"   pixel_values.shape: {inputs_seg['pixel_values'].shape}\")  # (1, 3, num_frames, 224, 224)\n    print(f\"   video_input_mask.shape: {video_input_mask_seg.shape}\")  # (1, num_tokens)\n    print(f\"   video_input_mask sum (should be {num_query_tokens}): {video_input_mask_seg.sum().item()}\")\n    # ëª¨ë¸ ì‹¤í–‰: ì„¸ê·¸ë¨¼íŠ¸ë³„ ìº¡ì…˜ ìƒì„±\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=80,\n            num_beams=4,\n            repetition_penalty=1.3,\n        )\n    caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n    \n    # ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„, ì¢…ë£Œ ì‹œê°„, ìº¡ì…˜ ì €ì¥\n    metadata.append({\n        \"start_time\": start_frame / fps,\n        \"end_time\": end_frame / fps,\n        \"caption\": caption\n    })\ncap.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['start_time']:.1f}s - {seg['end_time']:.1f}s: {seg['caption']}\")\n\n# Optional: ì €ì¥ (JSON íŒŒì¼)\nwith open(\"video_captions.json\", \"w\") as f:\n    json.dump(metadata, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"===========================ë¹„ë””ì˜¤ 5ì´ˆë§ˆë‹¤ ì„¤ëª…ìƒì„±=============================== kpyu/eilev-kpyu/eilev-blip2-flan-t5-xlì ìš©","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (FLANâ€‘T5 ê¸°ë°˜ EILeV)\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ (ì˜ˆ: ì²« 5ì´ˆ, ì´ˆë‹¹ 1í”„ë ˆì„)\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nprint(f\"Total frames: {frame_count}, FPS: {fps}\")\nnum_frames = int(fps * 5)\nframes = []\nindices = np.linspace(0, num_frames - 1, num=num_frames).astype(int)\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frames.append(frame)\ncap.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. í”„ë ˆì„ ì „ì²˜ë¦¬ (í…ì„œ ë³€í™˜ ë° ì¬ë°°ì—´)\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\ninputs_video = process(processor, video=frames_tensor, text=None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. ì…ë ¥ ì‹œí€€ìŠ¤ ë° video_input_mask êµ¬ì„± (í•™ìŠµ ì‹œ ì‚¬ìš©ëœ Q&A í˜•ì‹ ìœ ì§€)\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\ninput_ids_list = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids_list], dtype=torch.long).to(device)\nnum_tokens = input_ids.shape[1]\nvideo_input_mask_list = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n\ninputs = {\n    \"input_ids\": input_ids,\n    \"pixel_values\": inputs_video[\"pixel_values\"].to(device)\n}\n\n# ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° í¬ê¸° í™•ì¸\nprint(f\"ğŸ” input_ids.shape: {inputs['input_ids'].shape}\")         # (1, L)\nprint(f\"ğŸ” pixel_values.shape: {inputs['pixel_values'].shape}\")       # (1, 3, num_frames, 224, 224)\nprint(f\"ğŸ” video_input_mask.shape: {video_input_mask.shape}\")         # (1, L)\nprint(f\"ğŸ” video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. ëª¨ë¸ ì‹¤í–‰ ë° ìº¡ì…˜ ìƒì„± (ë””ì½”ë”© íŒŒë¼ë¯¸í„° ì¡°ì •)\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,\n        max_new_tokens=100,      # ë” ê¸´ ë‹µë³€ì„ ìœ„í•´ ëŠ˜ë¦¼\n        num_beams=6,             # ë¹” ì„œì¹˜ ì¦ê°€\n        temperature=0.7,         # ì˜¨ë„ ì¡°ì ˆë¡œ ë‹¤ì–‘ì„± í™•ë³´\n        repetition_penalty=1.1,  # ë°˜ë³µ ì–µì œ ì™„í™”\n    )\n\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n# í›„ì²˜ë¦¬: ì²« ë²ˆì§¸ Q&A ìŒë§Œ ì¶”ì¶œ\nfinal_caption = generated_text.split(\"\\n\")[0]\n\nprint(\"\\nğŸ¬ Video Description Generated:\")\nprint(final_caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['start_time']:.1f}s - {seg['end_time']:.1f}s: {seg['caption']}\")\n\n# Optional: ì €ì¥ (JSON íŒŒì¼)\nwith open(\"video_captions(flan-t5).json\", \"w\") as f:\n    json.dump(metadata, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\n\n# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (FLANâ€‘T5 ê¸°ë°˜ EILeV)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # FLANâ€‘T5 ê¸°ë°˜ ëª¨ë¸\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n# ëª¨ë¸ì€ ì´ë¯¸ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨ (ì¶”ê°€ .to(device)ëŠ” í•„ìš”ì—†ìŒ)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. ë¹„ë””ì˜¤(mp4) íŒŒì¼ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ (ì˜ˆ: ì²« 5ì´ˆ, ì´ˆë‹¹ 1í”„ë ˆì„)\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nprint(f\"Total frames: {frame_count}, FPS: {fps}\")\nnum_frames = int(fps * 5)  # 5ì´ˆ ë¶„ëŸ‰\nframes = []\nindices = np.linspace(0, num_frames - 1, num=num_frames).astype(int)\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frames.append(frame)\ncap.release()\n\n# ==================================================\n# 3. í”„ë ˆì„ ì „ì²˜ë¦¬: í…ì„œ ë³€í™˜ ë° ì¬ë°°ì—´ (ìµœì¢… shape: [1, 3, num_frames, 224, 224])\n# ==================================================\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\n# ==================================================\n# 4. ë¹„ë””ì˜¤ ì…ë ¥ ì „ì²˜ë¦¬: process() í•¨ìˆ˜ ì‚¬ìš©\n# ==================================================\ninputs_video = process(processor, video=frames_tensor, text=None)\n\n# ==================================================\n# 5. ì…ë ¥ ì‹œí€€ìŠ¤ ë° video_input_mask êµ¬ì„± (Q&A í˜•ì‹ ìœ ì§€)\n# ==================================================\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32\n\n# ì „ì²´ ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\ninput_ids_list = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids_list], dtype=torch.long).to(device)\nnum_tokens = input_ids.shape[1]\n\n# video_input_mask: [bos] â†’ False, ë‹¤ìŒ num_query_tokens â†’ True, ë‚˜ë¨¸ì§€ â†’ False\nvideo_input_mask_list = [False] + [True] * num_query_tokens + [False] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n\n# ==================================================\n# 6. ìµœì¢… ì…ë ¥ êµ¬ì„±\n# ==================================================\ninputs = {\n    \"input_ids\": input_ids,\n    \"pixel_values\": inputs_video[\"pixel_values\"].to(device)\n}\n\n# ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° í¬ê¸° í™•ì¸\nprint(f\"ğŸ” input_ids.shape: {inputs['input_ids'].shape}\")         # ì˜ˆ: (1, L)\nprint(f\"ğŸ” pixel_values.shape: {inputs['pixel_values'].shape}\")       # (1, 3, num_frames, 224, 224)\nprint(f\"ğŸ” video_input_mask.shape: {video_input_mask.shape}\")         # (1, L)\nprint(f\"ğŸ” video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\n\n# ==================================================\n# 7. ëª¨ë¸ ì‹¤í–‰ (ìº¡ì…˜ ìƒì„±) - ë””ì½”ë”© íŒŒë¼ë¯¸í„° ì¡°ì •\n# ==================================================\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,\n        max_new_tokens=100,      # ë” ê¸´ ë‹µë³€ ìƒì„±\n        num_beams=6,             # ë¹” ì„œì¹˜ í™•ëŒ€\n        temperature=0.7,         # ì˜¨ë„ ì¡°ì ˆ\n        repetition_penalty=1.1,  # ë°˜ë³µ ì–µì œ ì™„í™”\n    )\n\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\n# í›„ì²˜ë¦¬: ì²« ë²ˆì§¸ Q&A ìŒë§Œ ì‚¬ìš© (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\nfinal_caption = generated_text.split(\"\\n\")[0]\n\nprint(\"\\nğŸ¬ Video Description Generated:\")\nprint(final_caption)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"blip2-plant-t5 ì‚¬ìš© EILEV ë‚´ë¶€í•¨ìˆ˜ ì‚¬ìš©\n","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n# ìºê¸€ ì…ë ¥ ê²½ë¡œ: EILeV ì €ì¥ì†Œê°€ /kaggle/input/eilev-git/EILEV-main ì— ìˆë‹¤ê³  ê°€ì •\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom eilev.data.utils import generate_input_ids_and_labels_from_interleaved  # ë‚´ë¶€ ì…ë ¥ êµ¬ì„± í•¨ìˆ˜ (ì˜µì…˜)\nfrom PIL import Image\nimport json\nimport time\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILeV, OPT ê¸°ë°˜)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # ìºê¸€ì— ì—…ë¡œë“œëœ EILeV OPT ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# CPU/GPU í™˜ê²½ì— ë”°ë¼ quantization ì‚¬ìš© ì—¬ë¶€ ê²°ì • (ìºê¸€ì—ì„œëŠ” ë³´í†µ GPU ì‚¬ìš©)\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\n# GPU í™˜ê²½ì—ì„œ 8-bit ì–‘ìí™”ëœ ëª¨ë¸ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë˜ë¯€ë¡œ .to(device) ë¶ˆí•„ìš”\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 2. ë¹„ë””ì˜¤ ì „ì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\nprint(f\"Total frames: {total_frames}, FPS: {fps}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ==================================================\n# # 3. ê¸°ë³¸ ì…ë ¥ ì‹œí€€ìŠ¤ ë° video_input_mask êµ¬ì„±\n# #    (EILeV í•™ìŠµ ì‹œ ì‚¬ìš©í•œ Q&A í˜•ì‹ ìœ ì§€)\n# # ==================================================\n# prompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\n# prompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# bos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n# pad_token = tokenizer.pad_token_id\n# newline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n# num_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# # ì „ì²´ ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\n# base_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\n# base_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\n# num_tokens = base_input_ids.shape[1]\n\n# # video_input_mask: bos í† í° â†’ 0, ë‹¤ìŒ num_query_tokens (ë¹„ë””ì˜¤ ìë¦¬) â†’ 1, ë‚˜ë¨¸ì§€ (í…ìŠ¤íŠ¸ ë¶€ë¶„) â†’ 0\n# base_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\n# base_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #opt ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…ìƒì„± ì§ˆë¬¸ê³¼ ë‹µì€ í›„ì²˜ë¦¬ë¡œ ì œê±°\n# # ==================================================\n# # 4. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„±\n# # ==================================================\n# segment_duration = 5  # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\n# frames_per_segment = int(fps * segment_duration)\n# metadata = []  # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë©”íƒ€ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸\n\n# cap = cv2.VideoCapture(video_path)\n# current_segment = 0\n# start_time_overall = time.time()\n\n# while True:\n#     start_frame = current_segment * frames_per_segment\n#     if start_frame >= total_frames:\n#         break\n#     end_frame = min(start_frame + frames_per_segment, total_frames)\n#     segment_frames = []\n    \n#     # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ í”„ë ˆì„ ì¶”ì¶œ\n#     for i in range(start_frame, end_frame):\n#         cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n#         ret, frame = cap.read()\n#         if not ret:\n#             break\n#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#         segment_frames.append(frame)\n    \n#     if len(segment_frames) == 0:\n#         break\n    \n#     # í”„ë ˆì„ í…ì„œ ë³€í™˜: (num_frames, H, W, C) â†’ (1, 3, num_frames, H, W)\n#     segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n#     segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n#     segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n#     # EILeV ê³µì‹ process() í•¨ìˆ˜ ì‚¬ìš© (ì‹œê°„ ì •ë³´ ìœ ì§€)\n#     inputs_video = process(processor, video=segment_tensor, text=None)\n#     pixel_values = inputs_video[\"pixel_values\"].to(device)  # (1, 3, num_segment_frames, 224, 224)\n    \n#     # ìµœì¢… ì…ë ¥ êµ¬ì„±: ë™ì¼í•œ í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ video mask ì‚¬ìš©\n#     inputs_seg = {\n#         \"input_ids\": base_input_ids,\n#         \"pixel_values\": pixel_values\n#     }\n#     video_input_mask_seg = base_video_mask  # (1, num_tokens)\n    \n#     seg_start_time = start_frame / fps\n#     seg_end_time = end_frame / fps\n#     print(f\"Segment {current_segment} ({seg_start_time:.1f}s - {seg_end_time:.1f}s):\")\n#     print(f\"   input_ids.shape: {inputs_seg['input_ids'].shape}\")    # (1, num_tokens)\n#     print(f\"   pixel_values.shape: {inputs_seg['pixel_values'].shape}\")  # (1, 3, num_segment_frames, 224, 224)\n#     print(f\"   video_input_mask.shape: {video_input_mask_seg.shape}\")  # (1, num_tokens)\n#     print(f\"   video_input_mask sum (should be {num_query_tokens}): {video_input_mask_seg.sum().item()}\")\n    \n#     # ëª¨ë¸ ì‹¤í–‰: ì„¸ê·¸ë¨¼íŠ¸ë³„ ìº¡ì…˜ ìƒì„±\n#     with torch.no_grad():\n#         output = model.generate(\n#             input_ids=inputs_seg[\"input_ids\"],\n#             pixel_values=inputs_seg[\"pixel_values\"],\n#             video_input_mask=video_input_mask_seg,\n#             max_new_tokens=80,\n#             num_beams=4,\n#             repetition_penalty=1.3,\n#         )\n#     caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n#     print(f\"   Generated caption: {caption}\")\n    \n#     metadata.append({\n#         \"segment\": current_segment,\n#         \"start_time\": seg_start_time,\n#         \"end_time\": seg_end_time,\n#         \"caption\": caption\n#     })\n#     current_segment += 1\n\n# cap.release()\n# total_time = time.time() - start_time_overall\n# print(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n# with open(\"/kaggle/working/video_captions_opt.json\", \"w\", encoding=\"utf-8\") as f:\n#     json.dump(metadata, f, indent=4, ensure_ascii=False)\n# print(\"Result saved to 'output/video_captions_opt.json'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 3. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„±\n# ==================================================\nsegment_duration = 5  # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\nframes_per_segment = int(fps * segment_duration)\nmetadata = []  # ê° ì„¸ê·¸ë¨¼íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸\n\n# FLANâ€‘T5ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ ë³€ê²½: ë‹¨ì¼ ì„œìˆ í˜• í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n# (Q&A í˜•ì‹ì´ ì•„ë‹ˆë¼ ì „ì²´ ì„¤ëª… ì„œìˆ ì„ ìœ ë„)\nprompt = \"Describe in detail what actions are taking place in the video.\"\n# prompt_tokensëŠ” ë‚´ë¶€ í•¨ìˆ˜ì—ì„œ ìë™ ì²˜ë¦¬ë˜ë„ë¡ í…ìŠ¤íŠ¸ ì…ë ¥ì„ Noneìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n# (ë”°ë¼ì„œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ëŠ” process() í•¨ìˆ˜ ëŒ€ì‹  ë³„ë„ë¡œ input_idsë¥¼ êµ¬ì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)\n\n# ëŒ€ì‹ , EILeV ë‚´ë¶€ í•¨ìˆ˜ì—ì„œ ë¹„ë””ì˜¤ ë¶€ë¶„ë§Œ ì²˜ë¦¬í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n# ì•„ë˜ base_input_idsì™€ video_input_maskëŠ” EILeV í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í˜•ì‹ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# ê¸°ë³¸ ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\n# ì—¬ê¸°ì„œëŠ” prompt_tokens ëŒ€ì‹  FLANâ€‘T5ì— ë§ê²Œ ê°„ë‹¨í•œ ì„œìˆ í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n# ì˜ˆë¥¼ ë“¤ì–´, í”„ë¡¬í”„íŠ¸ì— \"Describe in detail what is happening in the video.\" ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token]\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\n# video_input_mask: bos í† í° â†’ 0, ë‹¤ìŒ num_query_tokens (ë¹„ë””ì˜¤ ìë¦¬) â†’ 1, ë‚˜ë¨¸ì§€ëŠ” 0\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 4. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„± (ì˜¤ë²„ë© ì ìš©)\n# ==================================================\n# ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ë° ì˜¤ë²„ë© ì„¤ì • (ì´ˆ ë‹¨ìœ„)\nsegment_duration = 5          # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\noverlap_duration = 2          # ì˜¤ë²„ë© ê¸¸ì´ (ì´ˆ)\n\nframes_per_segment = int(fps * segment_duration)\noverlap_frames = int(fps * overlap_duration)\n# ê° ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ í”„ë ˆì„: ì„¸ê·¸ë¨¼íŠ¸ ì „ì²´ ê¸¸ì´ì—ì„œ ì˜¤ë²„ë© ë¶€ë¶„ì„ ëº€ ê°’ë§Œí¼ì”© ì§„í–‰\nstep_frames = frames_per_segment - overlap_frames\n\nmetadata = []  # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ë©”íƒ€ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸\ncurrent_segment = 0\nstart_time_overall = time.time()\n\ncap = cv2.VideoCapture(video_path)\nwhile True:\n    start_frame = current_segment * step_frames\n    if start_frame >= total_frames:\n        break\n    # ê° ì„¸ê·¸ë¨¼íŠ¸ëŠ” start_frameë¶€í„° frames_per_segment ë§Œí¼ì˜ í”„ë ˆì„ì„ í¬í•¨\n    end_frame = min(start_frame + frames_per_segment, total_frames)\n    \n    # ì„¸ê·¸ë¨¼íŠ¸ì— í•´ë‹¹í•˜ëŠ” í”„ë ˆì„ ì¶”ì¶œ\n    segment_frames = []\n    for i in range(start_frame, end_frame):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        segment_frames.append(frame)\n    \n    if len(segment_frames) == 0:\n        break\n\n    # í”„ë ˆì„ì„ í…ì„œë¡œ ë³€í™˜: (num_frames, H, W, C) â†’ (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    # EILeV ê³µì‹ process() í•¨ìˆ˜ ì‚¬ìš© (text ì…ë ¥ì€ Noneìœ¼ë¡œ ì„¤ì •)\n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    # ìµœì¢… ì…ë ¥ êµ¬ì„±: base_input_idsì™€ base_video_mask ì‚¬ìš©\n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    seg_start_time = start_frame / fps\n    seg_end_time = end_frame / fps\n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    print(f\"   input_ids.shape: {inputs_seg['input_ids'].shape}\")\n    print(f\"   pixel_values.shape: {inputs_seg['pixel_values'].shape}\")\n    print(f\"   video_input_mask.shape: {video_input_mask_seg.shape}\")\n    print(f\"   video_input_mask sum (should be {num_query_tokens}): {video_input_mask_seg.sum().item()}\")\n    \n    # ëª¨ë¸ ì‹¤í–‰: ìº¡ì…˜ ìƒì„±\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,   # max_new_tokens ê°’ì„ ëŠ˜ë ¤ì„œ ë” ê¸´ ìº¡ì…˜ ìƒì„±\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    \n    current_segment += 1\n\ncap.release()\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:15:34.443132Z","iopub.execute_input":"2025-03-18T03:15:34.443455Z","iopub.status.idle":"2025-03-18T03:15:58.159689Z","shell.execute_reply.started":"2025-03-18T03:15:34.443428Z","shell.execute_reply":"2025-03-18T03:15:58.158449Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nCollecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=ccbe3d561d2756f7eecce89db960d19c225ac9ccb301521c8ddc9fbd09fe49b0\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=5b77018f12e696091fce374801958c847b417a8ca01969a8126539dcfe7e3117\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=de281f6542ee125aaa2b3bcaa56ca3a3b717ea134276d4275a5ede9dea3702b8\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.2.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\nCollecting decord\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: decord\nSuccessfully installed decord-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\n# ìºê¸€ ì…ë ¥ ê²½ë¡œ: EILeV ì €ì¥ì†Œê°€ /kaggle/input/eilev-git/EILEV-main ì— ìˆë‹¤ê³  ê°€ì •\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\n\nfrom collections import deque  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:20:57.064203Z","iopub.execute_input":"2025-03-18T03:20:57.064648Z","iopub.status.idle":"2025-03-18T03:20:57.069832Z","shell.execute_reply.started":"2025-03-18T03:20:57.064609Z","shell.execute_reply":"2025-03-18T03:20:57.068831Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILeV, OPT ê¸°ë°˜)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # ìºê¸€ì— ì—…ë¡œë“œëœ EILeV OPT ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:16:26.802496Z","iopub.execute_input":"2025-03-18T03:16:26.803101Z","iopub.status.idle":"2025-03-18T03:19:04.288576Z","shell.execute_reply.started":"2025-03-18T03:16:26.803077Z","shell.execute_reply":"2025-03-18T03:19:04.287905Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a191efc242ff4b5b88b5607056ba9042"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc203428aca1451a9c109ce18c29b78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27418104e85d4380bbfbd79b9103cb63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4991442af5ab40f1968ff11ad7ecdd60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc5f2d0065440b29c22075da370a95a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1184664c5c784250bb2ffa6e4b126ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c0a53ff806349ddbe71b175227d44ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee055a9a71d4021813722c92725364e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f01d16532401497cbf9e010b34b62e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f181c60f1da4b8882a1d6cacf82aace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ade9c29d8c44a7f8d22ec438e93189f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa4b077f0b14a329bae0a75b1299fd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b460d01666cc4a0fb6a6f0490e0643e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57b8da0114249d981e5d53fb99fdbeb"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ==================================================\n# 2. ì˜ìƒ ì½ê¸° (imageioë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # ìƒˆ ì˜ìƒ ê²½ë¡œ\n\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    fps = meta['fps']\n    print(f\"Imageio - FPS: {fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# ==================================================\n# 3. ê¸°ë³¸ ì…ë ¥ êµ¬ì„± (FLANâ€‘T5 ë‹¨ì¼ ì„œìˆ í˜• í”„ë¡¬í”„íŠ¸ ë°©ì‹)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# ì¶”ê°€í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì •ì˜ (ì›í•˜ëŠ” ì„¤ëª…ì„ ìœ ë„í•  ë¬¸êµ¬)\nprompt = \"Describe in detail what actions are taking place in the video, including dynamic movements like walking, running, or interacting with objects.\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# ê¸°ë³¸ ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ì´ìš©í•œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„±\n# ==================================================\nsegment_duration = 5    # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\noverlap_duration = 2    # ì˜¤ë²„ë© ê¸¸ì´ (ì´ˆ)\n\nframes_per_segment = int(fps * segment_duration)\noverlap_frames = int(fps * overlap_duration)\nstep_frames = frames_per_segment - overlap_frames\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# dequeë¥¼ ì‚¬ìš©í•˜ì—¬ sliding window êµ¬ì„± (ìµœëŒ€ frames_per_segment ê°œì˜ í”„ë ˆì„ ìœ ì§€)\nwindow = deque(maxlen=frames_per_segment)\nframe_idx = 0\n\n# ì´ˆê¸° ìœˆë„ìš° ì±„ìš°ê¸°\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i)\n        window.append(frame)\n        frame_idx = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # ì²« ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (í”„ë ˆì„ 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / fps\n    segment_frames = list(window)\n    \n    # í”„ë ˆì„ í…ì„œ ë³€í™˜: (num_frames, H, W, C) â†’ (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    current_segment += 1\n\n    # ì´í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬\n    while True:\n        count = 0\n        # step_frames ë§Œí¼ ìƒˆë¡œìš´ í”„ë ˆì„ ì¶”ê°€ (ìœˆë„ìš°ëŠ” ìë™ìœ¼ë¡œ ì˜¤ë˜ëœ í”„ë ˆì„ì„ ì œê±°)\n        while count < step_frames:\n            try:\n                frame = reader.get_data(frame_idx)\n                window.append(frame)\n                frame_idx += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (frame_idx - frames_per_segment) / fps\n        seg_end_time = frame_idx / fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        caption = generated_text.strip()\n        print(f\"   Generated caption: {caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:31:45.615095Z","iopub.execute_input":"2025-03-18T03:31:45.615447Z","iopub.status.idle":"2025-03-18T03:34:21.107507Z","shell.execute_reply.started":"2025-03-18T03:31:45.615422Z","shell.execute_reply":"2025-03-18T03:34:21.106181Z"}},"outputs":[{"name":"stdout","text":"Imageio - FPS: 29.97\nframes_per_segment: 149, step_frames: 90\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 5.0s\n   Generated caption: ji yoon-ah and ji kyung-hee in a park\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 3.0s - 8.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 6.0s - 11.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 9.0s - 14.0s\n   Generated caption: ji yeon-ah and ji yeon-soo are playing a game of go-go on the grass.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 12.0s - 17.0s\n   Generated caption: ji yoon-ah and ji yoon-ah are seen playing on a kick scooter in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 5: 15.0s - 20.0s\n   Generated caption: ji yeon-ah and ji yeon-soo in\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 6: 18.0s - 23.0s\n   Generated caption: ji yoon-ah and ji kyung-hee have a romantic dinner together\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-12f56433efd8>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msegment_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0minputs_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_video\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/eilev-git/EILEV-main/eilev/model/utils.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(processor, video, text)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip_2/processing_blip_2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mimage_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moutput_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m                 )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvalid_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/image_processing_blip.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             images = [\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/image_processing_blip.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             images = [\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/image_processing_blip.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         return resize(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;31m# PIL images are in the format (width, height)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducing_gap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreducing_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 )\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m     def reduce(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg\n\nimport sys\nfrom collections import deque\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio\n\n# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILEV, OPT ê¸°ë°˜)\n# ==================================================\n# ì•„ë˜ ëª¨ë¸ ì´ë¦„ì€ ì˜ˆì‹œì…ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš©í•˜ì‹œëŠ” OPT ê¸°ë°˜ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"  \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. ì˜ìƒ ì½ê¸° (imageioë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # ì˜ìƒ ê²½ë¡œ (ì›í•˜ëŠ” ì˜ìƒìœ¼ë¡œ ë³€ê²½)\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    fps = meta['fps']\n    print(f\"Imageio - FPS: {fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# ==================================================\n# 3. ê¸°ë³¸ ì…ë ¥ êµ¬ì„± (OPT ê¸°ë°˜ EILEVë¥¼ ìœ„í•œ ë‹¨ì¼ ì„œìˆ í˜• í”„ë¡¬í”„íŠ¸)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# ì¶”ê°€ í”„ë¡¬í”„íŠ¸: ë™ì  í–‰ë™ì„ ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ë„ë¡ ìœ ë„\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# ê¸°ë³¸ ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ì´ìš©í•œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„± (ì˜¤ë²„ë© ì ìš©)\n# ==================================================\nsegment_duration = 5    # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)\noverlap_duration = 2    # ì˜¤ë²„ë© ê¸¸ì´ (ì´ˆ)\n\nframes_per_segment = int(fps * segment_duration)\noverlap_frames = int(fps * overlap_duration)\nstep_frames = frames_per_segment - overlap_frames\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# dequeë¥¼ ì‚¬ìš©í•˜ì—¬ sliding window êµ¬ì„± (ìµœëŒ€ frames_per_segment ê°œì˜ í”„ë ˆì„ ìœ ì§€)\nwindow = deque(maxlen=frames_per_segment)\nframe_idx = 0\n\n# ì´ˆê¸° ìœˆë„ìš° ì±„ìš°ê¸°\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i)\n        window.append(frame)\n        frame_idx = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # ì²« ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (í”„ë ˆì„ 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / fps\n    segment_frames = list(window)\n    \n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    current_segment += 1\n\n    # ì´í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬\n    while True:\n        count = 0\n        while count < step_frames:\n            try:\n                frame = reader.get_data(frame_idx)\n                window.append(frame)\n                frame_idx += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (frame_idx - frames_per_segment) / fps\n        seg_end_time = frame_idx / fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        caption = generated_text.strip()\n        print(f\"   Generated caption: {caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:50:10.711110Z","iopub.execute_input":"2025-03-18T03:50:10.711501Z","iopub.status.idle":"2025-03-18T03:53:45.866124Z","shell.execute_reply.started":"2025-03-18T03:50:10.711471Z","shell.execute_reply":"2025-03-18T03:53:45.864517Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: pytorchvideo in /usr/local/lib/python3.10/dist-packages (0.1.5)\nRequirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.5.post20221221)\nRequirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (14.2.0)\nRequirement already satisfied: parameterized in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.9.0)\nRequirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.10)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nRequirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.1.8)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9137ca2bcf48c5b2f1459a487cb9b7"}},"metadata":{}},{"name":"stdout","text":"Imageio - FPS: 29.97\nframes_per_segment: 149, step_frames: 90\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 5.0s\n   Generated caption: A man and a woman are walking on a bridge. The man is wearing a baseball cap and the woman is wearing a hat. The man is talking to the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 3.0s - 8.0s\n   Generated caption: A man and a woman are playing a game of frisbee. The man throws the frisbee to the woman. The woman catches the frisbee and throws it to the man. The man catches the frisbee and throws it to the woman. The woman catches the frisbee and throws it to the man. The man catches the frisbee and throws it to the woman. The woman catches the frisbee and throws it to the man. The man catches the frisbee and throws it to the woman. The woman catches the frisbee and throws it to\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 6.0s - 11.0s\n   Generated caption: A group of people are playing a game of kite flying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 9.0s - 14.0s\n   Generated caption: A group of people are playing a game of kick scooter.\nExplanation: A group of people are playing a game of kick scooter.\nCategories\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 12.0s - 17.0s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-96359562dddb>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             output = model.generate(\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/eilev-git/EILEV-main/eilev/model/v2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, pixel_values, video_input_mask, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_input_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         outputs = self.language_model.generate(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2283\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2284\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids)\u001b[0m\n\u001b[1;32m    931\u001b[0m                 )\n\u001b[1;32m    932\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    934\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache, position_ids)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, position_ids)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# reuse k, v, self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fp16_weights\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul8bitLt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# Fast path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8_vectorwise_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mCAt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCAt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mint8_vectorwise_quant\u001b[0;34m(A, threshold)\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0moutlier_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_cuda_device_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:22:36.143765Z","iopub.execute_input":"2025-03-18T05:22:36.143968Z","iopub.status.idle":"2025-03-18T05:22:58.839108Z","shell.execute_reply.started":"2025-03-18T05:22:36.143948Z","shell.execute_reply":"2025-03-18T05:22:58.837999Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nCollecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=4856419f5ca7955b4b67b80861535e76ff0595b240a894a5f62197c13c05efb7\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=9cf4035d15573696a037fe8e0932847285e780c26112ab72b83aec2151915551\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=8308ef9e38a2b6303fab924cdd6b220244cdc4cf78d980ef863accfaa02600e3\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.2.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\nCollecting decord\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: decord\nSuccessfully installed decord-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport sys\n# ìºê¸€ ì…ë ¥ ê²½ë¡œ: EILeV ì €ì¥ì†Œê°€ /kaggle/input/eilev-git/EILEV-main ì— ìˆë‹¤ê³  ê°€ì •\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\nfrom collections import deque\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:24:09.066698Z","iopub.execute_input":"2025-03-18T05:24:09.066992Z","iopub.status.idle":"2025-03-18T05:24:09.245468Z","shell.execute_reply.started":"2025-03-18T05:24:09.066971Z","shell.execute_reply":"2025-03-18T05:24:09.244571Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (FLANâ€‘T5 ê¸°ë°˜ EILEV)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # ì‚¬ìš©í•˜ì‹œëŠ” ì²´í¬í¬ì¸íŠ¸ì— ë§ê²Œ ìˆ˜ì •\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:24:11.592869Z","iopub.execute_input":"2025-03-18T05:24:11.593195Z","iopub.status.idle":"2025-03-18T05:26:34.712658Z","shell.execute_reply.started":"2025-03-18T05:24:11.593166Z","shell.execute_reply":"2025-03-18T05:26:34.711693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9c81181db4549a19e3f84edd78bbfba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85cff720aaea4ceab8dcd001f0863ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed06e78dbe0e4042bac96a1810355ba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a49bf9decd64416a12ff230e8eeec38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f169a436354e00a4077b33a6eed6d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543b72e113f247fa8658c68c75fcea8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a58df450f594fa289bef1493b4f583f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da0282d6d9a04b66adb54a5157ebd1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0106445cbb5472c9ab51fb3ce46c5e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb2c63be77f4472ae7cd1edc0f92a37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73aec6cda3a849bca3f6369cf3109d2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d64e930b71b4898adf51fb873013e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541e761077c248ccbd94a3d7f5ede43b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fce58f95ab84c3b916384f63cef0c54"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ==================================================\n# 2. ì˜ìƒ ì½ê¸° (imageioë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # ìƒˆ ì˜ìƒ ê²½ë¡œ\n\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    original_fps = meta['fps']\n    print(f\"Imageio - Original FPS: {original_fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# ì›í•˜ëŠ” ìƒ˜í”Œë§ ì„¤ì •: ì´ˆë‹¹ 5í”„ë ˆì„\ndesired_fps = 5  \n# ì›ë³¸ fpsì—ì„œ ëª‡ í”„ë ˆì„ë§ˆë‹¤ ìƒ˜í”Œë§í• ì§€ ê³„ì‚° (ì˜ˆ: 29.97/5 â‰ˆ 6)\nsampling_interval = int(round(original_fps / desired_fps))\nprint(f\"Sampling interval (in original frames): {sampling_interval}\")\n\n# ==================================================\n# 3. ê¸°ë³¸ ì…ë ¥ êµ¬ì„± (EILEVê°€ í•™ìŠµì‹œí‚¨ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ë¦¬ í† í° ê°œìˆ˜\n\n# ê¸°ì¡´ EILEV í”„ë¡¬í”„íŠ¸\nprompt = \"Describe in detail what actions are taking place in the video.\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# ê¸°ë³¸ ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ì´ìš©í•œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„±\n# ==================================================\n# ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ë¥¼ 6ì´ˆ, ì˜¤ë²„ë©ì€ 2ì´ˆë¡œ ì„¤ì •\nsegment_duration = 6    # ì´ˆ ë‹¨ìœ„\noverlap_duration = 2    # ì´ˆ ë‹¨ìœ„\n\nframes_per_segment = int(desired_fps * segment_duration)  # ì˜ˆ: 5 fps * 6ì´ˆ = 30 í”„ë ˆì„\noverlap_frames = int(desired_fps * overlap_duration)        # ì˜ˆ: 5 fps * 2ì´ˆ = 10 í”„ë ˆì„\nstep_frames = frames_per_segment - overlap_frames           # ì˜ˆ: 30 - 10 = 20 í”„ë ˆì„\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# dequeë¥¼ ì‚¬ìš©í•˜ì—¬ sliding window êµ¬ì„± (ìµœëŒ€ frames_per_segment ê°œì˜ í”„ë ˆì„ ìœ ì§€)\nwindow = deque(maxlen=frames_per_segment)\nsample_index = 0  # ìƒ˜í”Œ ë‹¨ìœ„ ì¸ë±ìŠ¤ (ì‹¤ì œ í”„ë ˆì„ ë²ˆí˜¸ëŠ” sample_index * sampling_interval)\n\n# ì´ˆê¸° ìœˆë„ìš° ì±„ìš°ê¸° (desired_fps ê¸°ë°˜ ìƒ˜í”Œë§)\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i * sampling_interval)\n        window.append(frame)\n        sample_index = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # ì²« ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ìƒ˜í”Œ 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / desired_fps  # ì˜ˆ: 30 / 5 = 6ì´ˆ\n    segment_frames = list(window)\n    \n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    current_segment += 1\n\n    # ì´í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬\n    while True:\n        count = 0\n        # step_frames ë§Œí¼ ìƒˆë¡œìš´ í”„ë ˆì„ ì¶”ê°€ (ìœˆë„ìš°ëŠ” ìë™ìœ¼ë¡œ ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°)\n        while count < step_frames:\n            try:\n                frame = reader.get_data(sample_index * sampling_interval)\n                window.append(frame)\n                sample_index += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (sample_index - frames_per_segment) / desired_fps\n        seg_end_time = sample_index / desired_fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        caption = generated_text.strip()\n        print(f\"   Generated caption: {caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:58:49.167094Z","iopub.execute_input":"2025-03-18T05:58:49.167441Z","iopub.status.idle":"2025-03-18T06:00:49.433304Z","shell.execute_reply.started":"2025-03-18T05:58:49.167414Z","shell.execute_reply":"2025-03-18T06:00:49.431887Z"}},"outputs":[{"name":"stdout","text":"Imageio - Original FPS: 29.97\nSampling interval (in original frames): 6\nframes_per_segment: 30, step_frames: 20\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 6.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 4.0s - 10.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 8.0s - 14.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 12.0s - 18.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 16.0s - 22.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 5: 20.0s - 26.0s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a76205f07724>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             output = model.generate(\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/eilev-git/EILEV-main/eilev/model/v2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, pixel_values, video_input_mask, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_input_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         outputs = self.language_model.generate(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2283\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2284\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     ):\n\u001b[0;32m--> 675\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:03:56.031274Z","iopub.execute_input":"2025-03-18T10:03:56.031654Z","iopub.status.idle":"2025-03-18T10:04:18.296292Z","shell.execute_reply.started":"2025-03-18T10:03:56.031623Z","shell.execute_reply":"2025-03-18T10:04:18.295478Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nCollecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=9cf53af577c20bad6874380da60bd527dd044ffa355bf51959a597c7c82d0c81\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=4d2e8ad582a9606ac94ada9ea88bad8445124fccf0e68432337f7bc7715d1858\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=aa9f95a6f30626cf7ac24b8bf159f8985c301b4f5ea65d165b984f6f39a51c92\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.2.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\nCollecting decord\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: decord\nSuccessfully installed decord-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\n# ìºê¸€ ì…ë ¥ ê²½ë¡œ: EILeV ì €ì¥ì†Œê°€ /kaggle/input/eilev-git/EILEV-main ì— ìˆë‹¤ê³  ê°€ì •\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\nfrom collections import deque\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:04:38.458948Z","iopub.execute_input":"2025-03-18T10:04:38.459517Z","iopub.status.idle":"2025-03-18T10:04:38.463807Z","shell.execute_reply.started":"2025-03-18T10:04:38.459494Z","shell.execute_reply":"2025-03-18T10:04:38.462983Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==================================================\n# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (EILEV, OPT ê¸°ë°˜)\n# ==================================================\n# ì‹¤ì œ ì‚¬ìš©í•˜ì‹œëŠ” OPT ê¸°ë°˜ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:19:28.978778Z","iopub.execute_input":"2025-03-18T06:19:28.979193Z","iopub.status.idle":"2025-03-18T06:24:36.639073Z","shell.execute_reply.started":"2025-03-18T06:19:28.979156Z","shell.execute_reply":"2025-03-18T06:24:36.638332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abe77ebe14f4d6b8ae76201813d9ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/122k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b832ead706564ef5b93201d86d7a54d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b391aa1078e849cb9510535f32087a15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292fdb63a52c4f67a9b1aa4c65fdfc24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a25ddcf74946dd9c6731a896aaaa55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f1dd7a409c43acb6afd7e7c29a4779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/105M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d4f17abf3e4f3c9f9cd641a270bbfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d05a36b1f748348028202ae7daf4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a5f63b161349d9b9b04bca4b192739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4d250875044165a010560385c0c8a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"336889d8868d4e4ab93c778aeff9e0d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79a4b4984964deda0f0240ea0eb1077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8f18bb2c2f49ed941bb546d80307b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d5481bf044f48b5839779d29f5f0397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e84632bb0f402191e0fbd097ea4768"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ==================================================\n# 2. ì˜ìƒ ì½ê¸° (imageioë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # ì˜ìƒ ê²½ë¡œ\n\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    original_fps = meta['fps']\n    print(f\"Imageio - Original FPS: {original_fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# ì›í•˜ëŠ” ìƒ˜í”Œë§ ì„¤ì •: ì´ˆë‹¹ 15í”„ë ˆì„\ndesired_fps = 15  \nsampling_interval = int(round(original_fps / desired_fps))\nprint(f\"Sampling interval (in original frames): {sampling_interval}\")\n\n# ==================================================\n# 3. ê¸°ë³¸ ì…ë ¥ êµ¬ì„± (OPT ê¸°ë°˜ EILEV í”„ë¡¬í”„íŠ¸)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ìŠ¤íŠ¸ í† í° ê°œìˆ˜\n\n# ê¸°ì¡´ í”„ë¡¬í”„íŠ¸: í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ í˜•ì‹\nprompt = \" Describe in detail what actions are taking place in the video.:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# í›„ì²˜ë¦¬ í•¨ìˆ˜: ë¶ˆí•„ìš”í•œ ì ‘ë‘ì–´ ì œê±°\n# ==================================================\ndef clean_caption(text):\n    # ê° ì¤„ë§ˆë‹¤ \"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\" ë“± ì ‘ë‘ì–´ ì œê±°\n    lines = text.splitlines()\n    filtered = [line.strip() for line in lines if not any(line.strip().startswith(prefix) for prefix in \n                                                           [\"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\"])]\n    return \" \".join(filtered).strip()\n\n# ==================================================\n# í›„ì²˜ë¦¬ í•¨ìˆ˜: ì¤‘ë³µ ë¬¸ì¥ ì œê±°\n# ==================================================\ndef remove_duplicate_sentences(text):\n    # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n    sentences = re.split(r'\\.\\s*', text)\n    seen = set()\n    unique_sentences = []\n    for s in sentences:\n        s = s.strip()\n        if s and s not in seen:\n            seen.add(s)\n            unique_sentences.append(s)\n    # ì¤‘ë³µ ì œê±°í•œ ë¬¸ì¥ì„ ë‹¤ì‹œ ë§ˆì¹¨í‘œë¡œ ì´ì–´ ë¶™ì„\n    result = '. '.join(unique_sentences)\n    if result and not result.endswith('.'):\n        result += '.'\n    return result\n\n# ==================================================\n# 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ì´ìš©í•œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„± (ì˜¤ë²„ë© ì ìš©)\n# ==================================================\n# ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ë¥¼ 5ì´ˆ, ì˜¤ë²„ë©ì€ 1ì´ˆë¡œ ì„¤ì •\nsegment_duration = 5    # ì´ˆ ë‹¨ìœ„\noverlap_duration = 1    # ì´ˆ ë‹¨ìœ„\n\nframes_per_segment = int(desired_fps * segment_duration)   # 15 fps * 5ì´ˆ = 75 í”„ë ˆì„\noverlap_frames = int(desired_fps * overlap_duration)         # 15 fps * 1ì´ˆ = 15 í”„ë ˆì„\nstep_frames = frames_per_segment - overlap_frames            # 75 - 15 = 60 í”„ë ˆì„\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# dequeë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬ì„± (ìµœëŒ€ frames_per_segment ê°œì˜ í”„ë ˆì„ ìœ ì§€)\nwindow = deque(maxlen=frames_per_segment)\nsample_index = 0  # ì‹¤ì œ í”„ë ˆì„ ë²ˆí˜¸ = sample_index * sampling_interval\n\n# ì´ˆê¸° ìœˆë„ìš° ì±„ìš°ê¸° (desired_fps ê¸°ë°˜ ìƒ˜í”Œë§)\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i * sampling_interval)\n        window.append(frame)\n        sample_index = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # ì²« ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ìƒ˜í”Œ ì¸ë±ìŠ¤ 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / desired_fps  # ì˜ˆ: 30 / 6 = 5ì´ˆ\n    segment_frames = list(window)\n    \n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì ‘ë‘ì–´ ì œê±° í›„, ì¤‘ë³µ ë¬¸ì¥ ì œê±°\n    final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n    print(f\"   Generated caption: {final_caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": final_caption\n    })\n    current_segment += 1\n\n    # ì´í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬\n    while True:\n        count = 0\n        while count < step_frames:\n            try:\n                frame = reader.get_data(sample_index * sampling_interval)\n                window.append(frame)\n                sample_index += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (sample_index - frames_per_segment) / desired_fps\n        seg_end_time = sample_index / desired_fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n        print(f\"   Generated caption: {final_caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": final_caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_opt.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_opt.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:54:16.976047Z","iopub.execute_input":"2025-03-18T06:54:16.976412Z","iopub.status.idle":"2025-03-18T08:32:14.531947Z","shell.execute_reply.started":"2025-03-18T06:54:16.976379Z","shell.execute_reply":"2025-03-18T08:32:14.530787Z"}},"outputs":[{"name":"stdout","text":"Imageio - Original FPS: 29.97\nSampling interval (in original frames): 2\nframes_per_segment: 75, step_frames: 60\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 5.0s\n   Generated caption: A man and a woman are walking on a bridge. The man is wearing a baseball cap and the woman is wearing a hat. The man is talking to the woman. The woman is looking at the man. The man is looking at the woman. The woman is looking at the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 4.0s - 9.0s\n   Generated caption: A group of people are playing a game of kite flying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 8.0s - 13.0s\n   Generated caption: A group of people are playing a game of kick the can.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 12.0s - 17.0s\n   Generated caption: A group of people are riding a scooter. Categories.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 16.0s - 21.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a hat. The man is holding a fishing rod and the woman is holding a fishing pole. The man is looking at the woman and the woman is looking at the man. The woman is smiling and the man is smiling. The man is looking at the woman and the woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 5: 20.0s - 25.0s\n   Generated caption: A group of people are sitting around a table. They are eating some food. One of the people is holding a chopstick. The other person is holding a bowl of food. The third person is holding a bowl of food. The fourth person is holding a bowl of food. The fifth person is holding a bowl of food. The sixth person is holding a bowl of food. The seventh person is holding a bowl of food. The eighth person is holding a bowl of food. The ninth person is holding a bowl of food. The tenth person.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 6: 24.0s - 29.0s\n   Generated caption: A group of people are sitting around a fire pit. One of the people is using a cell phone. The other people are talking to each other. The other people.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 7: 28.0s - 33.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at his phone. The woman is looking at her phone.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 8: 32.0s - 37.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 9: 36.0s - 41.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 10: 40.0s - 45.0s\n   Generated caption: In the video, there is a man and a woman sitting in chairs. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 11: 44.0s - 49.0s\n   Generated caption: In the video, there are three people sitting in a lawn chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 12: 48.0s - 53.0s\n   Generated caption: The image shows a stream of water flowing through a forest.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 13: 52.0s - 57.0s\n   Generated caption: The image shows a stream of water flowing through a grassy area. Other: The image shows a stream of water flowing through a grassy area.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 14: 56.0s - 61.0s\n   Generated caption: The picture shows a group of people crossing a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 15: 60.0s - 65.0s\n   Generated caption: A man and a woman are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 16: 64.0s - 69.0s\n   Generated caption: A man and a woman are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 17: 68.0s - 73.0s\n   Generated caption: A group of people are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 18: 72.0s - 77.0s\n   Generated caption: A man and a woman are walking on a bridge. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 19: 76.0s - 81.0s\n   Generated caption: A man and a woman are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 20: 80.0s - 85.0s\n   Generated caption: A group of people are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 21: 84.0s - 89.0s\n   Generated caption: A group of people are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 22: 88.0s - 93.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 23: 92.0s - 97.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 24: 96.0s - 101.0s\n   Generated caption: A man is talking to a woman. Woman: A man is talking to a woman. Camer.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 25: 100.0s - 105.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 26: 104.0s - 109.0s\n   Generated caption: In the video, there are a lot of people in the park. There is a sign that says autumn camp. There are a lot of people in the park. There are.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 27: 108.0s - 113.0s\n   Generated caption: In the video, there are three people standing in front of a sign that says autumn camp. One of the people is holding a sign that says autumn camp. The second person is holding a sign that says autumn camp. The third person is holding a sign that says autumn camp.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 28: 112.0s - 117.0s\n   Generated caption: ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 29: 116.0s - 121.0s\n   Generated caption: ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 30: 120.0s - 125.0s\n   Generated caption: The video shows a group of people talking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 31: 124.0s - 129.0s\n   Generated caption: The two girls are looking at each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 32: 128.0s - 133.0s\n   Generated caption: The girl is looking at the camera. The girl is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 33: 132.0s - 137.0s\n   Generated caption: ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ï¿½.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 34: 136.0s - 141.0s\n   Generated caption: The woman is adjusting the hat on the man's head. The woman is adjusting the hat on the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 35: 140.0s - 145.0s\n   Generated caption: In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 36: 144.0s - 149.0s\n   Generated caption: The woman is looking at the sky.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 37: 148.0s - 153.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 38: 152.0s - 157.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 39: 156.0s - 161.0s\n   Generated caption: The video shows a man and a woman talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 40: 160.0s - 165.0s\n   Generated caption: In the video, there are a lot of people sitting in a room. There are a lot of people sitting in a room. There are a lot of people sitting in.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 41: 164.0s - 169.0s\n   Generated caption: The person in the video is giving a hug to a stuffed animal.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 42: 168.0s - 173.0s\n   Generated caption: The woman is sitting on the chair. The woman is looking at the stuffed animal. The woman is holding the stuffed animal. The woman is holding the stuffed.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 43: 172.0s - 177.0s\n   Generated caption: The woman is sitting on the chair. The woman is holding a green apple. The woman is smiling. The woman is looking at the camera. The woman is looking at the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 44: 176.0s - 181.0s\n   Generated caption: The girl is eating the doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 45: 180.0s - 185.0s\n   Generated caption: The woman is sitting on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 46: 184.0s - 189.0s\n   Generated caption: A woman is holding a piece of paper with a question mark on it. A.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 47: 188.0s - 193.0s\n   Generated caption: A woman is holding a doughnut in her hands. A woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 48: 192.0s - 197.0s\n   Generated caption: A woman is sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 49: 196.0s - 201.0s\n   Generated caption: A woman is holding a doughnut in her hands.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 50: 200.0s - 205.0s\n   Generated caption: The woman is holding a piece of bread in her hands.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 51: 204.0s - 209.0s\n   Generated caption: The woman is looking at a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 52: 208.0s - 213.0s\n   Generated caption: The woman is folding the paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 53: 212.0s - 217.0s\n   Generated caption: The woman is looking at the paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 54: 216.0s - 221.0s\n   Generated caption: A woman is holding a piece of paper in her hand. She is looking at the paper with a surprised look on her face. A woman is holding a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 55: 220.0s - 225.0s\n   Generated caption: A woman is looking at a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 56: 224.0s - 229.0s\n   Generated caption: A woman is sitting at a table. The woman is looking at a piece of paper. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 57: 228.0s - 233.0s\n   Generated caption: A woman is looking at a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 58: 232.0s - 237.0s\n   Generated caption: A woman is sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 59: 236.0s - 241.0s\n   Generated caption: The girl is reading a book.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 60: 240.0s - 245.0s\n   Generated caption: A group of people are sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 61: 244.0s - 249.0s\n   Generated caption: The woman is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 62: 248.0s - 253.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a jacket. The man is holding a piece of paper and the woman is holding a piece of paper. The man is looking at the woman and the woman is looking at the man. The man is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 63: 252.0s - 257.0s\n   Generated caption: A group of people are standing in a park. One of the people is holding a camera. The other people are looking at the camera. One of the people is looking at the camera and the other people are looking at the camera. This video is part of the following collections  Description: A group of people are standing in a park. Description: A group of people are standing in a.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 64: 256.0s - 261.0s\n   Generated caption: A man and a woman are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 65: 260.0s - 265.0s\n   Generated caption: ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 66: 264.0s - 269.0s\n   Generated caption: In the video, there is a man and a woman sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 67: 268.0s - 273.0s\n   Generated caption: The video shows a man and a woman talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 68: 272.0s - 277.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 69: 276.0s - 281.0s\n   Generated caption: A man and a woman are walking in the park. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 70: 280.0s - 285.0s\n   Generated caption: A man and a woman are walking in a park. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 71: 284.0s - 289.0s\n   Generated caption: ê³ ë§™ìŠµë‹ˆë‹¤  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 72: 288.0s - 293.0s\n   Generated caption: The video shows a group of people. Question.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 73: 292.0s - 297.0s\n   Generated caption: In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 74: 296.0s - 301.0s\n   Generated caption: In the video, there is a man and a woman. The man is holding a camera. The woman is looking at the camera. The man is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 75: 300.0s - 305.0s\n   Generated caption: The two girls are talking to each other. The girl on the left is looking at the girl on the right. The girl on the right is looking at the girl on the left. The girl on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 76: 304.0s - 309.0s\n   Generated caption: The two girls are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 77: 308.0s - 313.0s\n   Generated caption: ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 78: 312.0s - 317.0s\n   Generated caption: A group of people are standing in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 79: 316.0s - 321.0s\n   Generated caption: A man and a woman are talking to each other. A man and.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 80: 320.0s - 325.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 81: 324.0s - 329.0s\n   Generated caption: A man and a woman are walking together.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 82: 328.0s - 333.0s\n   Generated caption: In the video, there is a man and a woman talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 83: 332.0s - 337.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 84: 336.0s - 341.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 85: 340.0s - 345.0s\n   Generated caption: A group of people are sitting on the grass. One of the people is holding a bag. The other people are looking at the bag.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 86: 344.0s - 349.0s\n   Generated caption: The video shows a group of people.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 87: 348.0s - 353.0s\n   Generated caption: A group of people are walking in a park. Answer.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 88: 352.0s - 357.0s\n   Generated caption: A man and a woman are playing with a dog.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 89: 356.0s - 361.0s\n   Generated caption: In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 90: 360.0s - 365.0s\n   Generated caption: The video shows a group of people walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 91: 364.0s - 369.0s\n   Generated caption: A group of people are standing in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 92: 368.0s - 373.0s\n   Generated caption: The video shows a group of people talking to each other. Question.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 93: 372.0s - 377.0s\n   Generated caption: The video shows a group of people. The people in the video are smiling. The people in the video are looking at each other. The people in the video are laughing. The people in the video are.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 94: 376.0s - 381.0s\n   Generated caption: The scene shows a group of people. The scene shows.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 95: 380.0s - 385.0s\n   Generated caption: A group of people are standing in a grass field. They are looking at a dog. The dog is lying on the ground. The dog is looking at the people. The people are looking at the dog. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 96: 384.0s - 389.0s\n   Generated caption: The video shows a group of people talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 97: 388.0s - 393.0s\n   Generated caption: A man is talking to a woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 98: 392.0s - 397.0s\n   Generated caption: A group of people are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 99: 396.0s - 401.0s\n   Generated caption: A group of people are playing a game of frisbee in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 100: 400.0s - 405.0s\n   Generated caption: The video shows a group of people sitting around a table. The table is covered with a white table cloth. There are several chairs arranged around the table. One of the chairs has a woman sitting on it. The woman is holding a cup of coffee in her hand. A man is sitting on the chair next to the woman. The man is holding a cup of coffee in his hand. The man is wearing a white shirt and blue jeans. The man is also holding a cup of coffee in his hand. A woman is sitting on the chair next to the man. The woman is holding a cup.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 101: 404.0s - 409.0s\n   Generated caption: The video starts with a shot of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his right hand. He is looking at the camera with a smile on his face. The next shot is of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 102: 408.0s - 413.0s\n   Generated caption: A man and a woman are having a picnic in the park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 103: 412.0s - 417.0s\n   Generated caption: A group of people are playing a game of badminton.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 104: 416.0s - 421.0s\n   Generated caption: The two women are playing a game of tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 105: 420.0s - 425.0s\n   Generated caption: A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 106: 424.0s - 429.0s\n   Generated caption: A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 107: 428.0s - 433.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 108: 432.0s - 437.0s\n   Generated caption: The woman is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 109: 436.0s - 441.0s\n   Generated caption: The woman is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 110: 440.0s - 445.0s\n   Generated caption: A group of people are playing a game of tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 111: 444.0s - 449.0s\n   Generated caption: A man and a woman are playing a game of kite flying. A man and a woman are playing a.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 112: 448.0s - 453.0s\n   Generated caption: A man and a woman are playing a game of kite flying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 113: 452.0s - 457.0s\n   Generated caption: A man and a woman are playing a game of tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 114: 456.0s - 461.0s\n   Generated caption: A man is playing tennis with a woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 115: 460.0s - 465.0s\n   Generated caption: A man and a woman are having a picnic. The man is sitting on a chair. The woman is sitting on a chair. The man and the woman are having a picnic. The man and the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 116: 464.0s - 469.0s\n   Generated caption: A man and a woman are playing a game of tennis. The man is holding a tennis racket. The woman is holding a tennis ball. The man is hitting the tennis ball with the tennis racket. The woman is hitting the tennis ball with the tennis ball.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 117: 468.0s - 473.0s\n   Generated caption: A man and a woman are having a picnic in a park. The man is sitting on a chair and the woman is sitting on the ground. The man is holding a cup of coffee and the woman is holding a cup of tea. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 118: 472.0s - 477.0s\n   Generated caption: The woman is playing with the man's hat.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 119: 476.0s - 481.0s\n   Generated caption: The two women are playing a game of \"magnets\" on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 120: 480.0s - 485.0s\n   Generated caption: The man and the woman are playing with the toy. The man is holding the toy in his hand. The woman is looking at the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 121: 484.0s - 489.0s\n   Generated caption: The two women are playing a game of dominoes. The woman on the left is playing dominoes with the woman on the right. The woman on the right is playing dominoes with the woman on the left.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 122: 488.0s - 493.0s\n   Generated caption: The two women are playing a game of dominoes.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 123: 492.0s - 497.0s\n   Generated caption: A man and a woman are playing a game of dominoes. The man is holding the dominoes in his hands. The woman is sitting on a chair. The man is holding the dom.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 124: 496.0s - 501.0s\n   Generated caption: The two women are playing with the toy. One of them is looking at the toy and the other one is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 125: 500.0s - 505.0s\n   Generated caption: The girl is playing a game with the boy. The boy is playing a game with the girl.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 126: 504.0s - 509.0s\n   Generated caption: The two girls are playing a game. The girl on the right is playing a game with the girl on the left. The girl on the left is playing a game with the girl on the right. The girl on the right is playing a game.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 127: 508.0s - 513.0s\n   Generated caption: The two women are playing a game of dominoes.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 128: 512.0s - 517.0s\n   Generated caption: The girl is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 129: 516.0s - 521.0s\n   Generated caption: The two women are playing a game of tic-tac-toe.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 130: 520.0s - 525.0s\n   Generated caption: The two women are sitting on the table. One of them is looking at the other one. The other one is looking at the other one. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 131: 524.0s - 529.0s\n   Generated caption: A man and a woman are sitting on a bean bag chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 132: 528.0s - 533.0s\n   Generated caption: A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 133: 532.0s - 537.0s\n   Generated caption: The man is looking out the window. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 134: 536.0s - 541.0s\n   Generated caption: The man is looking out of the window.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 135: 540.0s - 545.0s\n   Generated caption: A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 136: 544.0s - 549.0s\n   Generated caption: The woman is sitting at the table. The woman is looking at the camera. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 137: 548.0s - 553.0s\n   Generated caption: A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 138: 552.0s - 557.0s\n   Generated caption: A man and a woman are sitting at a table. The man looks at the woman. The woman looks at the man. The woman looks at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 139: 556.0s - 561.0s\n   Generated caption: A man and a woman are looking at each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 140: 560.0s - 565.0s\n   Generated caption: The girl is sleeping in the chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 141: 564.0s - 569.0s\n   Generated caption: The woman is sleeping in the chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 142: 568.0s - 573.0s\n   Generated caption: A man and a woman are sitting on the ground. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 143: 572.0s - 577.0s\n   Generated caption: The woman is playing a game of badminton with her friend.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 144: 576.0s - 581.0s\n   Generated caption: A man and a woman are playing a game of frisbee in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 145: 580.0s - 585.0s\n   Generated caption: A group of people are playing a game of frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 146: 584.0s - 589.0s\n   Generated caption: The woman is throwing the frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 147: 588.0s - 593.0s\n   Generated caption: A man throws a frisbee to a woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 148: 592.0s - 597.0s\n   Generated caption: A man and a woman are swinging on a swing.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 149: 596.0s - 601.0s\n   Generated caption: A man and a woman are swinging on a swing.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 150: 600.0s - 605.0s\n   Generated caption: A man and a woman are sleeping on the ground. The man is holding the woman's hand. The woman is holding the man's hand.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 151: 604.0s - 609.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 152: 608.0s - 613.0s\n   Generated caption: The woman is sitting on the chair. The woman is holding a cup of tea. The woman is drinking tea.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 153: 612.0s - 617.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The woman is looking at the man. The man is looking at the woman. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 154: 616.0s - 621.0s\n   Generated caption: The girl is sitting on the chair. The girl is looking at something. The girl is drinking something. The girl is eating something. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 155: 620.0s - 625.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 156: 624.0s - 629.0s\n   Generated caption: The woman is sitting on a chair. The man is sitting on a chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 157: 628.0s - 633.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 158: 632.0s - 637.0s\n   Generated caption: The man is sitting on the chair. The woman is sitting on the chair. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 159: 636.0s - 641.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 160: 640.0s - 645.0s\n   Generated caption: A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 161: 644.0s - 649.0s\n   Generated caption: The man is sitting on the chair. The woman is sitting on the chair. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 162: 648.0s - 653.0s\n   Generated caption: A man and a woman are sitting at a picnic table. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 163: 652.0s - 657.0s\n   Generated caption: The girl is sitting on the chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 164: 656.0s - 661.0s\n   Generated caption: The girl is eating a cake. ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 165: 660.0s - 665.0s\n   Generated caption: The girl is eating a piece of cake. ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ï¿½.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 166: 664.0s - 669.0s\n   Generated caption: A man and a woman are sitting at a table. The man is looking at a book. The woman is looking at the man. The man is looking at the woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 167: 668.0s - 673.0s\n   Generated caption: The woman is looking at the book. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 168: 672.0s - 677.0s\n   Generated caption: The two women are sitting on the chairs. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 169: 676.0s - 681.0s\n   Generated caption: In the video, there is a man and a woman sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 170: 680.0s - 685.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 171: 684.0s - 689.0s\n   Generated caption: The video shows a group of people sitting around a table. One of the people is holding a piece of paper. Another person is looking at the paper. The third person is looking at the paper. The fourth person is looking at the paper. The fifth person is looking at the paper. The sixth person is looking at the paper. The seventh person is looking at the paper. The eighth person is looking at the paper. The ninth person is looking at the paper. The tenth person is looking at the paper. The eleventh person is looking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 172: 688.0s - 693.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 173: 692.0s - 697.0s\n   Generated caption: A group of people are sitting around a table. One of the people is holding a piece of paper. The other people are looking at the piece of paper. The person holding the piece of paper is looking at the people looking at the piece of paper. The person holding the piece of paper is looking at the people.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 174: 696.0s - 701.0s\n   Generated caption: The girl is sitting on the chair. The girl is holding a carrot. The girl is looking at the camera. The girl is smiling.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 175: 700.0s - 705.0s\n   Generated caption: The woman is looking at the paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 176: 704.0s - 709.0s\n   Generated caption: In the video, there are two women sitting in a tent. One of them is wearing a baseball cap and the other one is not wearing a baseball cap. The woman wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is not wearing the baseball cap is looking at the woman who is wearing the baseball cap. The woman who is wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is wearing the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 177: 708.0s - 713.0s\n   Generated caption: In the video, there is a man and a woman sitting on a bench. The man is wearing a baseball cap. The woman is wearing a baseball cap. The woman is wearing.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 178: 712.0s - 717.0s\n   Generated caption: A group of people are sitting in a tent. Question.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 179: 716.0s - 721.0s\n   Generated caption: The video shows a group of people sitting around a table. The video then cuts to a close-up of one of the people at the table. The person is holding a cup of tea. The video then cuts to a close-up of another person at the table. The person is also holding a cup of tea. The video then cuts to a close-up of a third person at the table. The video then cuts to a close-up of a fourth person at the table. The person is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 180: 720.0s - 725.0s\n   Generated caption: A man and a woman are standing in front of a sign that says \"Manto Market\".\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 181: 724.0s - 729.0s\n   Generated caption: The two women are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 182: 728.0s - 733.0s\n   Generated caption: A man and a woman are walking in a tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 183: 732.0s - 737.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 184: 736.0s - 741.0s\n   Generated caption: The woman is looking at something in the tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 185: 740.0s - 745.0s\n   Generated caption: The woman is looking at something in the tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 186: 744.0s - 749.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 187: 748.0s - 753.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 188: 752.0s - 757.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 189: 756.0s - 761.0s\n   Generated caption: A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 190: 760.0s - 765.0s\n   Generated caption: The two women are playing a game of badminton.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 191: 764.0s - 769.0s\n   Generated caption: A group of people are playing a game of frisbee in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 192: 768.0s - 773.0s\n   Generated caption: The woman is throwing the frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 193: 772.0s - 777.0s\n   Generated caption: A woman throws a frisbee in the air.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 194: 776.0s - 781.0s\n   Generated caption: The woman is sitting on the chair. The woman is holding a tennis ball. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 195: 780.0s - 785.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 196: 784.0s - 789.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at something on the table. The woman is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 197: 788.0s - 793.0s\n   Generated caption: The woman takes the camera from the man and looks at it.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 198: 792.0s - 797.0s\n   Generated caption: The two women look at each other. The woman on the right looks at the woman on the left. The woman on the left looks at the woman on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 199: 796.0s - 801.0s\n   Generated caption: The woman knits a sweater.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 200: 800.0s - 805.0s\n   Generated caption: The woman knits a sweater.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 201: 804.0s - 809.0s\n   Generated caption: The woman is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 202: 808.0s - 813.0s\n   Generated caption: The woman is sitting on the chair. The woman is looking at the table. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 203: 812.0s - 817.0s\n   Generated caption: The two women are looking at each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 204: 816.0s - 821.0s\n   Generated caption: The woman is looking at something. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 205: 820.0s - 825.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 206: 824.0s - 829.0s\n   Generated caption: A man and a woman are standing in front of a tent. The man is holding a camera. The woman is looking at the camera. The man is talking to the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 207: 828.0s - 833.0s\n   Generated caption: A man and a woman are sitting under an umbrella. The man is holding a gun and the woman is holding a knife. The man is pointing the gun at the woman. The woman is pointing the knife at the man. The woman is pointing the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 208: 832.0s - 837.0s\n   Generated caption: A group of people are standing under an umbrella. The umbrella is being held by a man and a woman. The umbrella is being held.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 209: 836.0s - 841.0s\n   Generated caption: The two girls are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 210: 840.0s - 845.0s\n   Generated caption: The two girls are hugging each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 211: 844.0s - 849.0s\n   Generated caption: The two women are talking to each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 212: 848.0s - 853.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 213: 852.0s - 857.0s\n   Generated caption: The woman looks at the woman's hand.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 214: 856.0s - 861.0s\n   Generated caption: A woman is sitting in a tent. Woman: A woman is sitting in a tent. Camer.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 215: 860.0s - 865.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 216: 864.0s - 869.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 217: 868.0s - 873.0s\n   Generated caption: A group of people are sitting in a tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 218: 872.0s - 877.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 219: 876.0s - 881.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 220: 880.0s - 885.0s\n   Generated caption: A man and a woman are playing a game of frisbee in a park. A man and a woman are.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 221: 884.0s - 889.0s\n   Generated caption: A man and a woman are sitting under an umbrella.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 222: 888.0s - 893.0s\n   Generated caption: A man and a woman are playing a game of frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 223: 892.0s - 897.0s\n   Generated caption: A man and a woman are sitting in a lawn chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 224: 896.0s - 901.0s\n   Generated caption: The woman is sitting in a lawn chair. The woman is holding a tennis racket. The woman is holding a tennis ball. The woman is holding a tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 225: 900.0s - 905.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 226: 904.0s - 909.0s\n   Generated caption: The woman is sitting on the chair. The woman is looking at something.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 227: 908.0s - 913.0s\n   Generated caption: The woman is kneeling down on the floor.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 228: 912.0s - 917.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 229: 916.0s - 921.0s\n   Generated caption: The two women are talking to each other. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 230: 920.0s - 925.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 231: 924.0s - 929.0s\n   Generated caption: A man and a woman are playing a game of frisbee. The man throws the frisbee to the woman. The woman catches the frisbee and throws it back to the man. The man catches the frisbee and throws it back to the woman. The woman catches the fr.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 232: 928.0s - 933.0s\n   Generated caption: The video shows a girl playing with a frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 233: 932.0s - 937.0s\n   Generated caption: In the video, a man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 234: 936.0s - 941.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 235: 940.0s - 945.0s\n   Generated caption: A group of people are sitting around a table. A woman is talking to a man. A man is talking to a woman. A.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 236: 944.0s - 949.0s\n   Generated caption: A woman is holding a fly with a net.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 237: 948.0s - 953.0s\n   Generated caption: A woman is sitting on a chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 238: 952.0s - 957.0s\n   Generated caption: A woman is eating an apple.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 239: 956.0s - 961.0s\n   Generated caption: A woman is sitting at a table. The woman is holding an apple. The woman is looking at the apple. The woman is holding the apple. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 240: 960.0s - 965.0s\n   Generated caption: The girl is eating a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 241: 964.0s - 969.0s\n   Generated caption: A woman is sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 242: 968.0s - 973.0s\n   Generated caption: A woman is holding a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 243: 972.0s - 977.0s\n   Generated caption: In the video, there is a girl who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 244: 976.0s - 981.0s\n   Generated caption: The girl is eating a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 245: 980.0s - 985.0s\n   Generated caption: The girl is playing with a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 246: 984.0s - 989.0s\n   Generated caption: A woman is eating a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 247: 988.0s - 993.0s\n   Generated caption: The woman is holding a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 248: 992.0s - 997.0s\n   Generated caption: The woman is sitting at the table. The woman is holding a piece of paper. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 249: 996.0s - 1001.0s\n   Generated caption: The girl is sitting on the table. The girl is eating an apple.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 250: 1000.0s - 1005.0s\n   Generated caption: In the video, there is a woman who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 251: 1004.0s - 1009.0s\n   Generated caption: In the video, there is a girl who is sitting on a chair. She is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 252: 1008.0s - 1013.0s\n   Generated caption: There are two women sitting at a table. One of the women is talking to the other woman. The other woman is listening to what the first woman is saying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 253: 1012.0s - 1017.0s\n   Generated caption: The girl is playing with the dog. The girl is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 254: 1016.0s - 1021.0s\n   Generated caption: The girl is tying the rope to the tree.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 255: 1020.0s - 1025.0s\n   Generated caption: A man and a woman are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 256: 1024.0s - 1029.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 257: 1028.0s - 1033.0s\n   Generated caption: The man is trying to cover his face with his hands. The man is trying to cover his face.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 258: 1032.0s - 1037.0s\n   Generated caption: The woman is walking on the bridge. Catch the latest news, live coverage and in-depth analyses from India and World. Follow us on Facebook and Twitter.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 259: 1036.0s - 1041.0s\n   Generated caption: Describe in detail what actions are taking place in the video?.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 260: 1040.0s - 1045.0s\n   Generated caption: Describe in detail what actions are taking place in the video. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 261: 1044.0s - 1049.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 262: 1048.0s - 1053.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 263: 1052.0s - 1057.0s\n   Generated caption: Describe in detail what actions are taking place in the video?.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 264: 1056.0s - 1061.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 265: 1060.0s - 1065.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 266: 1064.0s - 1069.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 267: 1068.0s - 1073.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 268: 1072.0s - 1077.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 269: 1076.0s - 1081.0s\n   Generated caption: Describe in detail what actions are taking place in the video. The girl is flying a kite.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 270: 1080.0s - 1085.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 271: 1084.0s - 1089.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 272: 1088.0s - 1093.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\nReached end of video during sliding window update.\nTotal segmentation and caption generation time: 5877.1s\nSegment 0 (0.0s - 5.0s): A man and a woman are walking on a bridge. The man is wearing a baseball cap and the woman is wearing a hat. The man is talking to the woman. The woman is looking at the man. The man is looking at the woman. The woman is looking at the.\nSegment 1 (4.0s - 9.0s): A group of people are playing a game of kite flying.\nSegment 2 (8.0s - 13.0s): A group of people are playing a game of kick the can.\nSegment 3 (12.0s - 17.0s): A group of people are riding a scooter. Categories.\nSegment 4 (16.0s - 21.0s): In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a hat. The man is holding a fishing rod and the woman is holding a fishing pole. The man is looking at the woman and the woman is looking at the man. The woman is smiling and the man is smiling. The man is looking at the woman and the woman is looking at.\nSegment 5 (20.0s - 25.0s): A group of people are sitting around a table. They are eating some food. One of the people is holding a chopstick. The other person is holding a bowl of food. The third person is holding a bowl of food. The fourth person is holding a bowl of food. The fifth person is holding a bowl of food. The sixth person is holding a bowl of food. The seventh person is holding a bowl of food. The eighth person is holding a bowl of food. The ninth person is holding a bowl of food. The tenth person.\nSegment 6 (24.0s - 29.0s): A group of people are sitting around a fire pit. One of the people is using a cell phone. The other people are talking to each other. The other people.\nSegment 7 (28.0s - 33.0s): A man and a woman are sitting on a bench. The man is looking at his phone. The woman is looking at her phone.\nSegment 8 (32.0s - 37.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 9 (36.0s - 41.0s): A man and a woman are sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\nSegment 10 (40.0s - 45.0s): In the video, there is a man and a woman sitting in chairs. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\nSegment 11 (44.0s - 49.0s): In the video, there are three people sitting in a lawn chair.\nSegment 12 (48.0s - 53.0s): The image shows a stream of water flowing through a forest.\nSegment 13 (52.0s - 57.0s): The image shows a stream of water flowing through a grassy area. Other: The image shows a stream of water flowing through a grassy area.\nSegment 14 (56.0s - 61.0s): The picture shows a group of people crossing a bridge.\nSegment 15 (60.0s - 65.0s): A man and a woman are walking on a bridge.\nSegment 16 (64.0s - 69.0s): A man and a woman are walking on a bridge.\nSegment 17 (68.0s - 73.0s): A group of people are walking on a bridge.\nSegment 18 (72.0s - 77.0s): A man and a woman are walking on a bridge. The man is looking at the woman. The woman is looking at the man.\nSegment 19 (76.0s - 81.0s): A man and a woman are walking on a bridge.\nSegment 20 (80.0s - 85.0s): A group of people are walking on a bridge.\nSegment 21 (84.0s - 89.0s): A group of people are walking on a bridge.\nSegment 22 (88.0s - 93.0s): A group of people are walking in a park.\nSegment 23 (92.0s - 97.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 24 (96.0s - 101.0s): A man is talking to a woman. Woman: A man is talking to a woman. Camer.\nSegment 25 (100.0s - 105.0s): A group of people are walking in a park.\nSegment 26 (104.0s - 109.0s): In the video, there are a lot of people in the park. There is a sign that says autumn camp. There are a lot of people in the park. There are.\nSegment 27 (108.0s - 113.0s): In the video, there are three people standing in front of a sign that says autumn camp. One of the people is holding a sign that says autumn camp. The second person is holding a sign that says autumn camp. The third person is holding a sign that says autumn camp.\nSegment 28 (112.0s - 117.0s): ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\nSegment 29 (116.0s - 121.0s): ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\nSegment 30 (120.0s - 125.0s): The video shows a group of people talking.\nSegment 31 (124.0s - 129.0s): The two girls are looking at each other.\nSegment 32 (128.0s - 133.0s): The girl is looking at the camera. The girl is.\nSegment 33 (132.0s - 137.0s): ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ï¿½.\nSegment 34 (136.0s - 141.0s): The woman is adjusting the hat on the man's head. The woman is adjusting the hat on the.\nSegment 35 (140.0s - 145.0s): In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\nSegment 36 (144.0s - 149.0s): The woman is looking at the sky.\nSegment 37 (148.0s - 153.0s): In the video, there is a man and a woman who are talking to each other.\nSegment 38 (152.0s - 157.0s): In the video, there is a man and a woman who are talking to each other.\nSegment 39 (156.0s - 161.0s): The video shows a man and a woman talking to each other.\nSegment 40 (160.0s - 165.0s): In the video, there are a lot of people sitting in a room. There are a lot of people sitting in a room. There are a lot of people sitting in.\nSegment 41 (164.0s - 169.0s): The person in the video is giving a hug to a stuffed animal.\nSegment 42 (168.0s - 173.0s): The woman is sitting on the chair. The woman is looking at the stuffed animal. The woman is holding the stuffed animal. The woman is holding the stuffed.\nSegment 43 (172.0s - 177.0s): The woman is sitting on the chair. The woman is holding a green apple. The woman is smiling. The woman is looking at the camera. The woman is looking at the.\nSegment 44 (176.0s - 181.0s): The girl is eating the doughnut.\nSegment 45 (180.0s - 185.0s): The woman is sitting on the table.\nSegment 46 (184.0s - 189.0s): A woman is holding a piece of paper with a question mark on it. A.\nSegment 47 (188.0s - 193.0s): A woman is holding a doughnut in her hands. A woman is.\nSegment 48 (192.0s - 197.0s): A woman is sitting at a table.\nSegment 49 (196.0s - 201.0s): A woman is holding a doughnut in her hands.\nSegment 50 (200.0s - 205.0s): The woman is holding a piece of bread in her hands.\nSegment 51 (204.0s - 209.0s): The woman is looking at a piece of paper.\nSegment 52 (208.0s - 213.0s): The woman is folding the paper.\nSegment 53 (212.0s - 217.0s): The woman is looking at the paper.\nSegment 54 (216.0s - 221.0s): A woman is holding a piece of paper in her hand. She is looking at the paper with a surprised look on her face. A woman is holding a piece of paper.\nSegment 55 (220.0s - 225.0s): A woman is looking at a piece of paper.\nSegment 56 (224.0s - 229.0s): A woman is sitting at a table. The woman is looking at a piece of paper. The.\nSegment 57 (228.0s - 233.0s): A woman is looking at a piece of paper.\nSegment 58 (232.0s - 237.0s): A woman is sitting at a table.\nSegment 59 (236.0s - 241.0s): The girl is reading a book.\nSegment 60 (240.0s - 245.0s): A group of people are sitting at a table.\nSegment 61 (244.0s - 249.0s): The woman is looking at the camera.\nSegment 62 (248.0s - 253.0s): In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a jacket. The man is holding a piece of paper and the woman is holding a piece of paper. The man is looking at the woman and the woman is looking at the man. The man is looking at.\nSegment 63 (252.0s - 257.0s): A group of people are standing in a park. One of the people is holding a camera. The other people are looking at the camera. One of the people is looking at the camera and the other people are looking at the camera. This video is part of the following collections  Description: A group of people are standing in a park. Description: A group of people are standing in a.\nSegment 64 (256.0s - 261.0s): A man and a woman are talking to each other.\nSegment 65 (260.0s - 265.0s): ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\nSegment 66 (264.0s - 269.0s): In the video, there is a man and a woman sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking.\nSegment 67 (268.0s - 273.0s): The video shows a man and a woman talking to each other.\nSegment 68 (272.0s - 277.0s): The girl is looking at the camera.\nSegment 69 (276.0s - 281.0s): A man and a woman are walking in the park. The man is looking at the woman. The woman is looking at the man.\nSegment 70 (280.0s - 285.0s): A man and a woman are walking in a park. The man is looking at the woman. The woman is looking at the man.\nSegment 71 (284.0s - 289.0s): ê³ ë§™ìŠµë‹ˆë‹¤  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€  ëŒ“ê¸€.\nSegment 72 (288.0s - 293.0s): The video shows a group of people. Question.\nSegment 73 (292.0s - 297.0s): In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\nSegment 74 (296.0s - 301.0s): In the video, there is a man and a woman. The man is holding a camera. The woman is looking at the camera. The man is looking at the camera.\nSegment 75 (300.0s - 305.0s): The two girls are talking to each other. The girl on the left is looking at the girl on the right. The girl on the right is looking at the girl on the left. The girl on the right.\nSegment 76 (304.0s - 309.0s): The two girls are talking to each other.\nSegment 77 (308.0s - 313.0s): ê³ ë§™ìŠµë‹ˆë‹¤  ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™ìŠµë‹ˆë‹¤ ê³ ë§™.\nSegment 78 (312.0s - 317.0s): A group of people are standing in a park.\nSegment 79 (316.0s - 321.0s): A man and a woman are talking to each other. A man and.\nSegment 80 (320.0s - 325.0s): A group of people are walking in a park.\nSegment 81 (324.0s - 329.0s): A man and a woman are walking together.\nSegment 82 (328.0s - 333.0s): In the video, there is a man and a woman talking to each other.\nSegment 83 (332.0s - 337.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 84 (336.0s - 341.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 85 (340.0s - 345.0s): A group of people are sitting on the grass. One of the people is holding a bag. The other people are looking at the bag.\nSegment 86 (344.0s - 349.0s): The video shows a group of people.\nSegment 87 (348.0s - 353.0s): A group of people are walking in a park. Answer.\nSegment 88 (352.0s - 357.0s): A man and a woman are playing with a dog.\nSegment 89 (356.0s - 361.0s): In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\nSegment 90 (360.0s - 365.0s): The video shows a group of people walking in a park.\nSegment 91 (364.0s - 369.0s): A group of people are standing in a park.\nSegment 92 (368.0s - 373.0s): The video shows a group of people talking to each other. Question.\nSegment 93 (372.0s - 377.0s): The video shows a group of people. The people in the video are smiling. The people in the video are looking at each other. The people in the video are laughing. The people in the video are.\nSegment 94 (376.0s - 381.0s): The scene shows a group of people. The scene shows.\nSegment 95 (380.0s - 385.0s): A group of people are standing in a grass field. They are looking at a dog. The dog is lying on the ground. The dog is looking at the people. The people are looking at the dog. The.\nSegment 96 (384.0s - 389.0s): The video shows a group of people talking to each other.\nSegment 97 (388.0s - 393.0s): A man is talking to a woman.\nSegment 98 (392.0s - 397.0s): A group of people are talking to each other.\nSegment 99 (396.0s - 401.0s): A group of people are playing a game of frisbee in a park.\nSegment 100 (400.0s - 405.0s): The video shows a group of people sitting around a table. The table is covered with a white table cloth. There are several chairs arranged around the table. One of the chairs has a woman sitting on it. The woman is holding a cup of coffee in her hand. A man is sitting on the chair next to the woman. The man is holding a cup of coffee in his hand. The man is wearing a white shirt and blue jeans. The man is also holding a cup of coffee in his hand. A woman is sitting on the chair next to the man. The woman is holding a cup.\nSegment 101 (404.0s - 409.0s): The video starts with a shot of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his right hand. He is looking at the camera with a smile on his face. The next shot is of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his.\nSegment 102 (408.0s - 413.0s): A man and a woman are having a picnic in the park.\nSegment 103 (412.0s - 417.0s): A group of people are playing a game of badminton.\nSegment 104 (416.0s - 421.0s): The two women are playing a game of tennis.\nSegment 105 (420.0s - 425.0s): A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\nSegment 106 (424.0s - 429.0s): A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\nSegment 107 (428.0s - 433.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 108 (432.0s - 437.0s): The woman is playing with the toy.\nSegment 109 (436.0s - 441.0s): The woman is playing with the toy.\nSegment 110 (440.0s - 445.0s): A group of people are playing a game of tennis.\nSegment 111 (444.0s - 449.0s): A man and a woman are playing a game of kite flying. A man and a woman are playing a.\nSegment 112 (448.0s - 453.0s): A man and a woman are playing a game of kite flying.\nSegment 113 (452.0s - 457.0s): A man and a woman are playing a game of tennis.\nSegment 114 (456.0s - 461.0s): A man is playing tennis with a woman.\nSegment 115 (460.0s - 465.0s): A man and a woman are having a picnic. The man is sitting on a chair. The woman is sitting on a chair. The man and the woman are having a picnic. The man and the.\nSegment 116 (464.0s - 469.0s): A man and a woman are playing a game of tennis. The man is holding a tennis racket. The woman is holding a tennis ball. The man is hitting the tennis ball with the tennis racket. The woman is hitting the tennis ball with the tennis ball.\nSegment 117 (468.0s - 473.0s): A man and a woman are having a picnic in a park. The man is sitting on a chair and the woman is sitting on the ground. The man is holding a cup of coffee and the woman is holding a cup of tea. The man is looking at the woman. The woman is looking at the man.\nSegment 118 (472.0s - 477.0s): The woman is playing with the man's hat.\nSegment 119 (476.0s - 481.0s): The two women are playing a game of \"magnets\" on the table.\nSegment 120 (480.0s - 485.0s): The man and the woman are playing with the toy. The man is holding the toy in his hand. The woman is looking at the toy.\nSegment 121 (484.0s - 489.0s): The two women are playing a game of dominoes. The woman on the left is playing dominoes with the woman on the right. The woman on the right is playing dominoes with the woman on the left.\nSegment 122 (488.0s - 493.0s): The two women are playing a game of dominoes.\nSegment 123 (492.0s - 497.0s): A man and a woman are playing a game of dominoes. The man is holding the dominoes in his hands. The woman is sitting on a chair. The man is holding the dom.\nSegment 124 (496.0s - 501.0s): The two women are playing with the toy. One of them is looking at the toy and the other one is playing with the toy.\nSegment 125 (500.0s - 505.0s): The girl is playing a game with the boy. The boy is playing a game with the girl.\nSegment 126 (504.0s - 509.0s): The two girls are playing a game. The girl on the right is playing a game with the girl on the left. The girl on the left is playing a game with the girl on the right. The girl on the right is playing a game.\nSegment 127 (508.0s - 513.0s): The two women are playing a game of dominoes.\nSegment 128 (512.0s - 517.0s): The girl is playing with the toy.\nSegment 129 (516.0s - 521.0s): The two women are playing a game of tic-tac-toe.\nSegment 130 (520.0s - 525.0s): The two women are sitting on the table. One of them is looking at the other one. The other one is looking at the other one. The.\nSegment 131 (524.0s - 529.0s): A man and a woman are sitting on a bean bag chair. The man is looking at the woman. The woman is looking at the man.\nSegment 132 (528.0s - 533.0s): A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\nSegment 133 (532.0s - 537.0s): The man is looking out the window. The man is.\nSegment 134 (536.0s - 541.0s): The man is looking out of the window.\nSegment 135 (540.0s - 545.0s): A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\nSegment 136 (544.0s - 549.0s): The woman is sitting at the table. The woman is looking at the camera. The woman is.\nSegment 137 (548.0s - 553.0s): A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\nSegment 138 (552.0s - 557.0s): A man and a woman are sitting at a table. The man looks at the woman. The woman looks at the man. The woman looks at.\nSegment 139 (556.0s - 561.0s): A man and a woman are looking at each other.\nSegment 140 (560.0s - 565.0s): The girl is sleeping in the chair.\nSegment 141 (564.0s - 569.0s): The woman is sleeping in the chair.\nSegment 142 (568.0s - 573.0s): A man and a woman are sitting on the ground. The man is looking at the woman. The woman is looking at the man.\nSegment 143 (572.0s - 577.0s): The woman is playing a game of badminton with her friend.\nSegment 144 (576.0s - 581.0s): A man and a woman are playing a game of frisbee in a park.\nSegment 145 (580.0s - 585.0s): A group of people are playing a game of frisbee.\nSegment 146 (584.0s - 589.0s): The woman is throwing the frisbee.\nSegment 147 (588.0s - 593.0s): A man throws a frisbee to a woman.\nSegment 148 (592.0s - 597.0s): A man and a woman are swinging on a swing.\nSegment 149 (596.0s - 601.0s): A man and a woman are swinging on a swing.\nSegment 150 (600.0s - 605.0s): A man and a woman are sleeping on the ground. The man is holding the woman's hand. The woman is holding the man's hand.\nSegment 151 (604.0s - 609.0s): The woman is sitting on the chair. The man is sitting on the chair. The man is.\nSegment 152 (608.0s - 613.0s): The woman is sitting on the chair. The woman is holding a cup of tea. The woman is drinking tea.\nSegment 153 (612.0s - 617.0s): The woman is sitting on the chair. The man is sitting on the chair. The woman is looking at the man. The man is looking at the woman. The man is.\nSegment 154 (616.0s - 621.0s): The girl is sitting on the chair. The girl is looking at something. The girl is drinking something. The girl is eating something. The.\nSegment 155 (620.0s - 625.0s): The woman is sitting on the chair. The man is sitting on the chair. The man is.\nSegment 156 (624.0s - 629.0s): The woman is sitting on a chair. The man is sitting on a chair. The man is.\nSegment 157 (628.0s - 633.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 158 (632.0s - 637.0s): The man is sitting on the chair. The woman is sitting on the chair. The woman is.\nSegment 159 (636.0s - 641.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 160 (640.0s - 645.0s): A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\nSegment 161 (644.0s - 649.0s): The man is sitting on the chair. The woman is sitting on the chair. The woman is.\nSegment 162 (648.0s - 653.0s): A man and a woman are sitting at a picnic table. The man is looking at the woman. The woman is looking at the man.\nSegment 163 (652.0s - 657.0s): The girl is sitting on the chair.\nSegment 164 (656.0s - 661.0s): The girl is eating a cake. ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§ ê¸°ì¡´  ì •ë§.\nSegment 165 (660.0s - 665.0s): The girl is eating a piece of cake. ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ì†Œë…„ë‹¨  ìˆ™ï¿½.\nSegment 166 (664.0s - 669.0s): A man and a woman are sitting at a table. The man is looking at a book. The woman is looking at the man. The man is looking at the woman.\nSegment 167 (668.0s - 673.0s): The woman is looking at the book. The woman is.\nSegment 168 (672.0s - 677.0s): The two women are sitting on the chairs. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\nSegment 169 (676.0s - 681.0s): In the video, there is a man and a woman sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking.\nSegment 170 (680.0s - 685.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 171 (684.0s - 689.0s): The video shows a group of people sitting around a table. One of the people is holding a piece of paper. Another person is looking at the paper. The third person is looking at the paper. The fourth person is looking at the paper. The fifth person is looking at the paper. The sixth person is looking at the paper. The seventh person is looking at the paper. The eighth person is looking at the paper. The ninth person is looking at the paper. The tenth person is looking at the paper. The eleventh person is looking.\nSegment 172 (688.0s - 693.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 173 (692.0s - 697.0s): A group of people are sitting around a table. One of the people is holding a piece of paper. The other people are looking at the piece of paper. The person holding the piece of paper is looking at the people looking at the piece of paper. The person holding the piece of paper is looking at the people.\nSegment 174 (696.0s - 701.0s): The girl is sitting on the chair. The girl is holding a carrot. The girl is looking at the camera. The girl is smiling.\nSegment 175 (700.0s - 705.0s): The woman is looking at the paper.\nSegment 176 (704.0s - 709.0s): In the video, there are two women sitting in a tent. One of them is wearing a baseball cap and the other one is not wearing a baseball cap. The woman wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is not wearing the baseball cap is looking at the woman who is wearing the baseball cap. The woman who is wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is wearing the.\nSegment 177 (708.0s - 713.0s): In the video, there is a man and a woman sitting on a bench. The man is wearing a baseball cap. The woman is wearing a baseball cap. The woman is wearing.\nSegment 178 (712.0s - 717.0s): A group of people are sitting in a tent. Question.\nSegment 179 (716.0s - 721.0s): The video shows a group of people sitting around a table. The video then cuts to a close-up of one of the people at the table. The person is holding a cup of tea. The video then cuts to a close-up of another person at the table. The person is also holding a cup of tea. The video then cuts to a close-up of a third person at the table. The video then cuts to a close-up of a fourth person at the table. The person is.\nSegment 180 (720.0s - 725.0s): A man and a woman are standing in front of a sign that says \"Manto Market\".\nSegment 181 (724.0s - 729.0s): The two women are talking to each other.\nSegment 182 (728.0s - 733.0s): A man and a woman are walking in a tent.\nSegment 183 (732.0s - 737.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 184 (736.0s - 741.0s): The woman is looking at something in the tent.\nSegment 185 (740.0s - 745.0s): The woman is looking at something in the tent.\nSegment 186 (744.0s - 749.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\nSegment 187 (748.0s - 753.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 188 (752.0s - 757.0s): The girl is looking at the camera.\nSegment 189 (756.0s - 761.0s): A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\nSegment 190 (760.0s - 765.0s): The two women are playing a game of badminton.\nSegment 191 (764.0s - 769.0s): A group of people are playing a game of frisbee in a park.\nSegment 192 (768.0s - 773.0s): The woman is throwing the frisbee.\nSegment 193 (772.0s - 777.0s): A woman throws a frisbee in the air.\nSegment 194 (776.0s - 781.0s): The woman is sitting on the chair. The woman is holding a tennis ball. The woman is.\nSegment 195 (780.0s - 785.0s): The woman is sitting on the chair. The man is sitting on the chair. The man is.\nSegment 196 (784.0s - 789.0s): A man and a woman are sitting in a tent. The man is looking at something on the table. The woman is looking at something on the table.\nSegment 197 (788.0s - 793.0s): The woman takes the camera from the man and looks at it.\nSegment 198 (792.0s - 797.0s): The two women look at each other. The woman on the right looks at the woman on the left. The woman on the left looks at the woman on the right.\nSegment 199 (796.0s - 801.0s): The woman knits a sweater.\nSegment 200 (800.0s - 805.0s): The woman knits a sweater.\nSegment 201 (804.0s - 809.0s): The woman is looking at something on the table.\nSegment 202 (808.0s - 813.0s): The woman is sitting on the chair. The woman is looking at the table. The woman is.\nSegment 203 (812.0s - 817.0s): The two women are looking at each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\nSegment 204 (816.0s - 821.0s): The woman is looking at something. The woman is.\nSegment 205 (820.0s - 825.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\nSegment 206 (824.0s - 829.0s): A man and a woman are standing in front of a tent. The man is holding a camera. The woman is looking at the camera. The man is talking to the camera.\nSegment 207 (828.0s - 833.0s): A man and a woman are sitting under an umbrella. The man is holding a gun and the woman is holding a knife. The man is pointing the gun at the woman. The woman is pointing the knife at the man. The woman is pointing the.\nSegment 208 (832.0s - 837.0s): A group of people are standing under an umbrella. The umbrella is being held by a man and a woman. The umbrella is being held.\nSegment 209 (836.0s - 841.0s): The two girls are talking to each other.\nSegment 210 (840.0s - 845.0s): The two girls are hugging each other.\nSegment 211 (844.0s - 849.0s): The two women are talking to each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\nSegment 212 (848.0s - 853.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 213 (852.0s - 857.0s): The woman looks at the woman's hand.\nSegment 214 (856.0s - 861.0s): A woman is sitting in a tent. Woman: A woman is sitting in a tent. Camer.\nSegment 215 (860.0s - 865.0s): The girl is looking at the camera.\nSegment 216 (864.0s - 869.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 217 (868.0s - 873.0s): A group of people are sitting in a tent.\nSegment 218 (872.0s - 877.0s): The woman is sitting on the chair. The man is sitting on the chair. The woman is.\nSegment 219 (876.0s - 881.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 220 (880.0s - 885.0s): A man and a woman are playing a game of frisbee in a park. A man and a woman are.\nSegment 221 (884.0s - 889.0s): A man and a woman are sitting under an umbrella.\nSegment 222 (888.0s - 893.0s): A man and a woman are playing a game of frisbee.\nSegment 223 (892.0s - 897.0s): A man and a woman are sitting in a lawn chair. The man is looking at the woman. The woman is looking at the man.\nSegment 224 (896.0s - 901.0s): The woman is sitting in a lawn chair. The woman is holding a tennis racket. The woman is holding a tennis ball. The woman is holding a tennis.\nSegment 225 (900.0s - 905.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 226 (904.0s - 909.0s): The woman is sitting on the chair. The woman is looking at something.\nSegment 227 (908.0s - 913.0s): The woman is kneeling down on the floor.\nSegment 228 (912.0s - 917.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\nSegment 229 (916.0s - 921.0s): The two women are talking to each other. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\nSegment 230 (920.0s - 925.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 231 (924.0s - 929.0s): A man and a woman are playing a game of frisbee. The man throws the frisbee to the woman. The woman catches the frisbee and throws it back to the man. The man catches the frisbee and throws it back to the woman. The woman catches the fr.\nSegment 232 (928.0s - 933.0s): The video shows a girl playing with a frisbee.\nSegment 233 (932.0s - 937.0s): In the video, a man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\nSegment 234 (936.0s - 941.0s): A group of people are walking in a park.\nSegment 235 (940.0s - 945.0s): A group of people are sitting around a table. A woman is talking to a man. A man is talking to a woman. A.\nSegment 236 (944.0s - 949.0s): A woman is holding a fly with a net.\nSegment 237 (948.0s - 953.0s): A woman is sitting on a chair.\nSegment 238 (952.0s - 957.0s): A woman is eating an apple.\nSegment 239 (956.0s - 961.0s): A woman is sitting at a table. The woman is holding an apple. The woman is looking at the apple. The woman is holding the apple. The.\nSegment 240 (960.0s - 965.0s): The girl is eating a doughnut.\nSegment 241 (964.0s - 969.0s): A woman is sitting at a table.\nSegment 242 (968.0s - 973.0s): A woman is holding a piece of paper.\nSegment 243 (972.0s - 977.0s): In the video, there is a girl who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\nSegment 244 (976.0s - 981.0s): The girl is eating a doughnut.\nSegment 245 (980.0s - 985.0s): The girl is playing with a doughnut.\nSegment 246 (984.0s - 989.0s): A woman is eating a doughnut.\nSegment 247 (988.0s - 993.0s): The woman is holding a piece of paper.\nSegment 248 (992.0s - 997.0s): The woman is sitting at the table. The woman is holding a piece of paper. The.\nSegment 249 (996.0s - 1001.0s): The girl is sitting on the table. The girl is eating an apple.\nSegment 250 (1000.0s - 1005.0s): In the video, there is a woman who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\nSegment 251 (1004.0s - 1009.0s): In the video, there is a girl who is sitting on a chair. She is looking at something on the table.\nSegment 252 (1008.0s - 1013.0s): There are two women sitting at a table. One of the women is talking to the other woman. The other woman is listening to what the first woman is saying.\nSegment 253 (1012.0s - 1017.0s): The girl is playing with the dog. The girl is.\nSegment 254 (1016.0s - 1021.0s): The girl is tying the rope to the tree.\nSegment 255 (1020.0s - 1025.0s): A man and a woman are walking in a park.\nSegment 256 (1024.0s - 1029.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 257 (1028.0s - 1033.0s): The man is trying to cover his face with his hands. The man is trying to cover his face.\nSegment 258 (1032.0s - 1037.0s): The woman is walking on the bridge. Catch the latest news, live coverage and in-depth analyses from India and World. Follow us on Facebook and Twitter.\nSegment 259 (1036.0s - 1041.0s): Describe in detail what actions are taking place in the video?.\nSegment 260 (1040.0s - 1045.0s): Describe in detail what actions are taking place in the video. The man is looking at the woman. The woman is looking at the man.\nSegment 261 (1044.0s - 1049.0s): Describe in detail what actions are taking place in the video.\nSegment 262 (1048.0s - 1053.0s): Describe in detail what actions are taking place in the video.\nSegment 263 (1052.0s - 1057.0s): Describe in detail what actions are taking place in the video?.\nSegment 264 (1056.0s - 1061.0s): The girl is looking at the camera.\nSegment 265 (1060.0s - 1065.0s): Describe in detail what actions are taking place in the video.\nSegment 266 (1064.0s - 1069.0s): Describe in detail what actions are taking place in the video.\nSegment 267 (1068.0s - 1073.0s): Describe in detail what actions are taking place in the video.\nSegment 268 (1072.0s - 1077.0s): Describe in detail what actions are taking place in the video.\nSegment 269 (1076.0s - 1081.0s): Describe in detail what actions are taking place in the video. The girl is flying a kite.\nSegment 270 (1080.0s - 1085.0s): Describe in detail what actions are taking place in the video.\nSegment 271 (1084.0s - 1089.0s): Describe in detail what actions are taking place in the video.\nSegment 272 (1088.0s - 1093.0s): Describe in detail what actions are taking place in the video.\nResult saved to '/kaggle/working/video_captions_opt.json'\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# # ==================================================\n# # 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (FLANâ€‘T5 ê¸°ë°˜ EILEV)\n# # ==================================================\n# model_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # ì‚¬ìš©í•˜ì‹œëŠ” ì²´í¬í¬ì¸íŠ¸ì— ë§ê²Œ ìˆ˜ì •\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# quant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\n# model = VideoBlipForConditionalGeneration.from_pretrained(\n#     model_name,\n#     device_map=\"auto\",\n#     quantization_config=quant_config,\n#     torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n# )\n# processor = AutoProcessor.from_pretrained(model_name)\n# tokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:04:38.465089Z","iopub.execute_input":"2025-03-18T10:04:38.465282Z","iopub.status.idle":"2025-03-18T10:07:07.677986Z","shell.execute_reply.started":"2025-03-18T10:04:38.465265Z","shell.execute_reply":"2025-03-18T10:07:07.677315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"511b8584dfd2498fa80ff9f1ba2a3417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe7005ffb154a8988642b51dd7128f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0005355c574636ab63f9e18828e310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2e2a36fdb4467c92124b3712d05af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3121153a2a914543aaa4b792b1c1f3d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e2954f84e043fe8c68d017fb74e4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1131971e95740e39dc153413fbcf70f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee55f16c06d41d9905d0f935b0ef475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73d1cde0a244fe6aa528793bdc91244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8906f726fb0d4af78b5d7aedf3947750"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5460ef0ab73d4b97a7f41bd893169ce2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e58e61bf065463bbca3ee7ff3ac2aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2a1ecb61b44935bcbe3f7f08885831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6199e1f6a4564f4aa6a1156872cfa895"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# # ==================================================\n# # 2. ì˜ìƒ ì½ê¸° (imageioë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©)\n# # ==================================================\n# video_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # ì˜ìƒ ê²½ë¡œ\n\n# try:\n#     reader = imageio.get_reader(video_path, 'ffmpeg')\n#     meta = reader.get_meta_data()\n#     original_fps = meta['fps']\n#     print(f\"Imageio - Original FPS: {original_fps}\")\n# except Exception as e:\n#     raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# # ì›í•˜ëŠ” ìƒ˜í”Œë§ ì„¤ì •: ì´ˆë‹¹ 15í”„ë ˆì„\n# desired_fps = 15  \n# sampling_interval = int(round(original_fps / desired_fps))\n# print(f\"Sampling interval (in original frames): {sampling_interval}\")\n\n# # ==================================================\n# # 3. ê¸°ë³¸ ì…ë ¥ êµ¬ì„± (OPT ê¸°ë°˜ EILEV í”„ë¡¬í”„íŠ¸)\n# # ==================================================\n# bos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n# pad_token = tokenizer.pad_token_id\n# newline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n# num_query_tokens = 32  # ì˜ìƒ ê´€ë ¨ ì¿¼ìŠ¤íŠ¸ í† í° ê°œìˆ˜\n\n# # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸: í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ í˜•ì‹\n# prompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\n# prompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# base_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\n# base_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\n# num_tokens = base_input_ids.shape[1]\n\n# base_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\n# base_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# # ==================================================\n# # í›„ì²˜ë¦¬ í•¨ìˆ˜: ë¶ˆí•„ìš”í•œ ì ‘ë‘ì–´ ì œê±°\n# # ==================================================\n# def clean_caption(text):\n#     # ê° ì¤„ë§ˆë‹¤ \"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\" ë“± ì ‘ë‘ì–´ ì œê±°\n#     lines = text.splitlines()\n#     filtered = [line.strip() for line in lines if not any(line.strip().startswith(prefix) for prefix in \n#                                                            [\"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\"])]\n#     return \" \".join(filtered).strip()\n\n# # ==================================================\n# # í›„ì²˜ë¦¬ í•¨ìˆ˜: ì¤‘ë³µ ë¬¸ì¥ ì œê±°\n# # ==================================================\n# def remove_duplicate_sentences(text):\n#     # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n#     sentences = re.split(r'\\.\\s*', text)\n#     seen = set()\n#     unique_sentences = []\n#     for s in sentences:\n#         s = s.strip()\n#         if s and s not in seen:\n#             seen.add(s)\n#             unique_sentences.append(s)\n#     # ì¤‘ë³µ ì œê±°í•œ ë¬¸ì¥ì„ ë‹¤ì‹œ ë§ˆì¹¨í‘œë¡œ ì´ì–´ ë¶™ì„\n#     result = '. '.join(unique_sentences)\n#     if result and not result.endswith('.'):\n#         result += '.'\n#     return result\n\n# # ==================================================\n# # 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ì´ìš©í•œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë° ìº¡ì…˜ ìƒì„± (ì˜¤ë²„ë© ì ìš©)\n# # ==================================================\n# # ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ë¥¼ 5ì´ˆ, ì˜¤ë²„ë©ì€ 1ì´ˆë¡œ ì„¤ì •\n# segment_duration = 5    # ì´ˆ ë‹¨ìœ„\n# overlap_duration = 1    # ì´ˆ ë‹¨ìœ„\n\n# frames_per_segment = int(desired_fps * segment_duration)   # 15 fps * 5ì´ˆ = 75 í”„ë ˆì„\n# overlap_frames = int(desired_fps * overlap_duration)         # 15 fps * 1ì´ˆ = 15 í”„ë ˆì„\n# step_frames = frames_per_segment - overlap_frames            # 75 - 15 = 60 í”„ë ˆì„\n\n# print(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\n# metadata = []\n# current_segment = 0\n# start_time_overall = time.time()\n\n# # dequeë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬ì„± (ìµœëŒ€ frames_per_segment ê°œì˜ í”„ë ˆì„ ìœ ì§€)\n# window = deque(maxlen=frames_per_segment)\n# sample_index = 0  # ì‹¤ì œ í”„ë ˆì„ ë²ˆí˜¸ = sample_index * sampling_interval\n\n# # ì´ˆê¸° ìœˆë„ìš° ì±„ìš°ê¸° (desired_fps ê¸°ë°˜ ìƒ˜í”Œë§)\n# print(\"Preloading initial window...\")\n# for i in range(frames_per_segment):\n#     try:\n#         frame = reader.get_data(i * sampling_interval)\n#         window.append(frame)\n#         sample_index = i + 1\n#     except Exception:\n#         break\n\n# if len(window) < frames_per_segment:\n#     print(\"Not enough frames to form a full segment. Exiting.\")\n# else:\n#     # ì²« ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ìƒ˜í”Œ ì¸ë±ìŠ¤ 0 ~ frames_per_segment-1)\n#     seg_start_time = 0.0\n#     seg_end_time = frames_per_segment / desired_fps  # ì˜ˆ: 30 / 6 = 5ì´ˆ\n#     segment_frames = list(window)\n    \n#     segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n#     segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n#     segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n#     inputs_video = process(processor, video=segment_tensor, text=None)\n#     pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n#     inputs_seg = {\n#         \"input_ids\": base_input_ids,\n#         \"pixel_values\": pixel_values\n#     }\n#     video_input_mask_seg = base_video_mask\n    \n#     print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n#     with torch.no_grad():\n#         output = model.generate(\n#             input_ids=inputs_seg[\"input_ids\"],\n#             pixel_values=inputs_seg[\"pixel_values\"],\n#             video_input_mask=video_input_mask_seg,\n#             max_new_tokens=120,\n#             num_beams=6,\n#             repetition_penalty=1.2,\n#         )\n#     generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n#     # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì ‘ë‘ì–´ ì œê±° í›„, ì¤‘ë³µ ë¬¸ì¥ ì œê±°\n#     final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n#     print(f\"   Generated caption: {final_caption}\")\n    \n#     metadata.append({\n#         \"segment\": current_segment,\n#         \"start_time\": seg_start_time,\n#         \"end_time\": seg_end_time,\n#         \"caption\": final_caption\n#     })\n#     current_segment += 1\n\n#     # ì´í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬\n#     while True:\n#         count = 0\n#         while count < step_frames:\n#             try:\n#                 frame = reader.get_data(sample_index * sampling_interval)\n#                 window.append(frame)\n#                 sample_index += 1\n#                 count += 1\n#             except Exception:\n#                 break\n#         if count < step_frames:\n#             print(\"Reached end of video during sliding window update.\")\n#             break\n        \n#         seg_start_time = (sample_index - frames_per_segment) / desired_fps\n#         seg_end_time = sample_index / desired_fps\n#         segment_frames = list(window)\n        \n#         segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n#         segment_tensor = segment_tensor.unsqueeze(0)\n#         segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n#         inputs_video = process(processor, video=segment_tensor, text=None)\n#         pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n#         inputs_seg = {\n#             \"input_ids\": base_input_ids,\n#             \"pixel_values\": pixel_values\n#         }\n#         video_input_mask_seg = base_video_mask\n        \n#         print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n#         with torch.no_grad():\n#             output = model.generate(\n#                 input_ids=inputs_seg[\"input_ids\"],\n#                 pixel_values=inputs_seg[\"pixel_values\"],\n#                 video_input_mask=video_input_mask_seg,\n#                 max_new_tokens=120,\n#                 num_beams=6,\n#                 repetition_penalty=1.2,\n#             )\n#         generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n#         final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n#         print(f\"   Generated caption: {final_caption}\")\n        \n#         metadata.append({\n#             \"segment\": current_segment,\n#             \"start_time\": seg_start_time,\n#             \"end_time\": seg_end_time,\n#             \"caption\": final_caption\n#         })\n#         current_segment += 1\n\n# total_time = time.time() - start_time_overall\n# print(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# # ==================================================\n# # 5. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥\n# # ==================================================\n# for seg in metadata:\n#     print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\n# with open(\"/kaggle/working/video_captions_opt.json\", \"w\", encoding=\"utf-8\") as f:\n#     json.dump(metadata, f, indent=4, ensure_ascii=False)\n# print(\"Result saved to '/kaggle/working/video_captions_opt.json'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-18T11:29:19.177Z"}},"outputs":[],"execution_count":null}]}